{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: How to Run Cross-Agent Evaluations\n",
    "\n",
    "This notebook documents the procedures for running MCP evaluations with different coding agents.\n",
    "\n",
    "**Purpose:** Reproducibility documentation for cross-agent comparison experiments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We use the `metacoder` framework to evaluate different coding agents (Goose, Claude Code, Gemini) on literature retrieval tasks using various MCP servers.\n",
    "\n",
    "**Evaluation Framework:** metacoder (https://github.com/Your-Org/metacoder)\n",
    "\n",
    "**Test Configuration:** `project/literature_mcp_eval_config.yaml` (base config)\n",
    "\n",
    "**Agents Tested:**\n",
    "- Goose (goose-cli)\n",
    "- Claude Code (claude CLI)\n",
    "- Gemini (gemini-cli) - planned\n",
    "\n",
    "**MCP Servers:**\n",
    "- artl (ARTL MCP)\n",
    "- simple-pubmed (simple PubMed MCP)\n",
    "- biomcp (BioMCP)\n",
    "- pubmed-mcp (PubMed MCP from chrismannina)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Files\n",
    "\n",
    "### Base Configuration: `project/literature_mcp_eval_config.yaml`\n",
    "\n",
    "This file defines:\n",
    "- **Coders:** `goose: {}`\n",
    "- **Models:** `claude-4-sonnet` (Anthropic's claude-sonnet-4-20250514)\n",
    "- **MCP Servers:** Connection details for all 4 MCPs\n",
    "- **Test Cases:** 25 literature retrieval tasks\n",
    "\n",
    "### Agent-Specific Configurations\n",
    "\n",
    "- `literature_mcp_eval_config.yaml` - Goose\n",
    "- `literature_mcp_eval_config_claude.yaml` - Claude Code\n",
    "- `literature_mcp_eval_config_gemini.yaml` - Gemini\n",
    "\n",
    "**Key Difference:** The `coders:` section changes to specify the agent:\n",
    "\n",
    "```yaml\n",
    "# Goose\n",
    "coders:\n",
    "  goose: {}\n",
    "\n",
    "# Claude Code\n",
    "coders:\n",
    "  claude: {}\n",
    "\n",
    "# Gemini\n",
    "coders:\n",
    "  gemini: {}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Server Configurations\n",
    "\n",
    "### 1. ARTL (artl-mcp)\n",
    "```yaml\n",
    "artl:\n",
    "  name: artl\n",
    "  command: uvx\n",
    "  args: [artl-mcp]\n",
    "```\n",
    "\n",
    "### 2. Simple PubMed\n",
    "```yaml\n",
    "simple-pubmed:\n",
    "  name: pubmed\n",
    "  command: uvx\n",
    "  args: [mcp-simple-pubmed]\n",
    "  env:\n",
    "    PUBMED_EMAIL: ctparker@lbl.gov\n",
    "```\n",
    "\n",
    "### 3. BioMCP\n",
    "```yaml\n",
    "biomcp:\n",
    "  name: biomcp\n",
    "  command: uv\n",
    "  args: [\"run\", \"--with\", \"biomcp-python\", \"biomcp\", \"run\"]\n",
    "```\n",
    "\n",
    "### 4. PubMed MCP (chrismannina)\n",
    "```yaml\n",
    "pubmed-mcp:\n",
    "  name: pubmed-mcp\n",
    "  command: uv\n",
    "  args: [\"run\", \"--with\", \"git+https://github.com/chrismannina/pubmed-mcp@main\", \"-m\", \"src.main\"]\n",
    "  env:\n",
    "    PUBMED_API_KEY: \"01eec0a16472164c6d69163bd28368311808\"\n",
    "```\n",
    "\n",
    "**Note:** Each MCP is tested independently (4 separate runs per test case).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Goose Evaluation\n",
    "\n",
    "### Prerequisites\n",
    "```bash\n",
    "# Install goose\n",
    "pip install goose-ai  # or appropriate installation method\n",
    "\n",
    "# Set API keys\n",
    "export ANTHROPIC_API_KEY=$(cat ~/anthropic.key)\n",
    "export OPENAI_API_KEY=$(cat ~/openai.key)\n",
    "```\n",
    "\n",
    "### Run Script: `run_goose_eval_fixed.sh`\n",
    "```bash\n",
    "#!/bin/bash\n",
    "cd /Users/jtr4v/PythonProject/mcp_literature_eval\n",
    "export OPENAI_API_KEY=$(cat ~/openai.key)\n",
    "export ANTHROPIC_API_KEY=$(cat ~/anthropic.key)\n",
    "rm -f results/compare_agents/goose_20251103.yaml\n",
    "uv run metacoder eval project/literature_mcp_eval_config.yaml \\\n",
    "  -o results/compare_agents/goose_20251103.yaml\n",
    "```\n",
    "\n",
    "### Execute\n",
    "```bash\n",
    "chmod +x run_goose_eval_fixed.sh\n",
    "./run_goose_eval_fixed.sh\n",
    "```\n",
    "\n",
    "### Results\n",
    "- **Output:** `results/compare_agents/goose_20251103.yaml`\n",
    "- **Test Cases:** 25 cases × 4 MCP servers = 100 evaluations\n",
    "- **Duration:** ~2-3 hours (depending on API response times)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Claude Code Evaluation\n",
    "\n",
    "### Prerequisites\n",
    "```bash\n",
    "# Install Claude Code CLI\n",
    "# (installation method depends on distribution)\n",
    "\n",
    "# Set API key\n",
    "export ANTHROPIC_API_KEY=$(cat ~/anthropic.key)\n",
    "```\n",
    "\n",
    "### Run Script: `run_claude_eval.sh` (create this)\n",
    "```bash\n",
    "#!/bin/bash\n",
    "cd /Users/jtr4v/PythonProject/mcp_literature_eval\n",
    "export ANTHROPIC_API_KEY=$(cat ~/anthropic.key)\n",
    "rm -f results/compare_agents/claude_$(date +%Y%m%d).yaml\n",
    "uv run metacoder eval project/literature_mcp_eval_config_claude.yaml \\\n",
    "  -o results/compare_agents/claude_$(date +%Y%m%d).yaml\n",
    "```\n",
    "\n",
    "### Execute\n",
    "```bash\n",
    "chmod +x run_claude_eval.sh\n",
    "./run_claude_eval.sh\n",
    "```\n",
    "\n",
    "### Results\n",
    "- **Output:** `results/compare_agents/claude_YYYYMMDD.yaml`\n",
    "- **Test Cases:** 25 cases × 4 MCP servers = 100 evaluations\n",
    "- **Duration:** ~2-3 hours\n",
    "\n",
    "**Note:** The Oct 31 run used: `results/compare_agents/claude_20251031.yaml`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Gemini Evaluation (Planned)\n",
    "\n",
    "### Prerequisites\n",
    "```bash\n",
    "# Install Gemini CLI\n",
    "# Set API key\n",
    "export GOOGLE_API_KEY=$(cat ~/google.key)\n",
    "```\n",
    "\n",
    "### Run Script: `run_gemini_eval.sh` (to be created)\n",
    "```bash\n",
    "#!/bin/bash\n",
    "cd /Users/jtr4v/PythonProject/mcp_literature_eval\n",
    "export GOOGLE_API_KEY=$(cat ~/google.key)\n",
    "rm -f results/compare_agents/gemini_$(date +%Y%m%d).yaml\n",
    "uv run metacoder eval project/literature_mcp_eval_config_gemini.yaml \\\n",
    "  -o results/compare_agents/gemini_$(date +%Y%m%d).yaml\n",
    "```\n",
    "\n",
    "### Execute\n",
    "```bash\n",
    "chmod +x run_gemini_eval.sh\n",
    "./run_gemini_eval.sh\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Workflow\n",
    "\n",
    "After collecting results from multiple agents:\n",
    "\n",
    "### 1. Update Analysis Notebook\n",
    "Edit `experiment_1_cross_agent_analysis.ipynb` to include new result files:\n",
    "\n",
    "```python\n",
    "result_files = {\n",
    "    'goose': '../results/compare_agents/goose_20251103.yaml',\n",
    "    'claude': '../results/compare_agents/claude_20251031.yaml',\n",
    "    'gemini': '../results/compare_agents/gemini_YYYYMMDD.yaml',  # Add when ready\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Run Analysis\n",
    "```bash\n",
    "cd notebook\n",
    "uv run jupyter nbconvert --execute --to notebook --inplace \\\n",
    "  experiment_1_cross_agent_analysis.ipynb\n",
    "```\n",
    "\n",
    "### 3. View Results\n",
    "```bash\n",
    "# Generated figures\n",
    "ls -la ../results/figures/exp1_*.png\n",
    "\n",
    "# Results summary\n",
    "cat ../notes/EXPERIMENT_1_RESULTS.md\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Goose MCP Extension Issues\n",
    "\n",
    "**Problem:** Goose fails with `pubmed-mcp` and `simple-pubmed` using `--with-extension`\n",
    "\n",
    "**Symptoms:**\n",
    "```\n",
    "Error: Command '[...goose', 'run', '-t', '...', '--with-extension', '...']'\n",
    "returned non-zero exit status 1.\n",
    "```\n",
    "\n",
    "**Diagnosis:**\n",
    "```bash\n",
    "# Test manually\n",
    "goose run -t \"What is PMID:12345?\" --with-extension \"uvx mcp-simple-pubmed\"\n",
    "```\n",
    "\n",
    "**Known Issues:**\n",
    "- Goose's `--with-extension` may have compatibility issues with certain MCP launch commands\n",
    "- The `uv run --with git+...` syntax may not work correctly as an extension\n",
    "\n",
    "**Resolution:** See `notes/test_goose_extension_fix.sh` for debugging steps\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case Structure\n",
    "\n",
    "Each test case includes:\n",
    "\n",
    "```yaml\n",
    "- name: PMID_28027860_Full_Text\n",
    "  group: \"Text extraction\"\n",
    "  metrics:\n",
    "  - CorrectnessMetric\n",
    "  input: \"What is the first sentence of section 2 in PMID:28027860?\"\n",
    "  expected_output: \"Even though many of NFLE's core features...\"\n",
    "  threshold: 0.9\n",
    "```\n",
    "\n",
    "**Test Case Groups:**\n",
    "1. Text extraction (9 cases)\n",
    "2. Metadata (8 cases)\n",
    "3. Table/Figure/Legend extraction (4 cases)\n",
    "4. Summarization (2 cases)\n",
    "5. Supplementary material (1 case)\n",
    "6. Publication status (1 case)\n",
    "\n",
    "**Total:** 25 test cases × 4 MCP servers = 100 evaluations per agent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "### CorrectnessMetric\n",
    "\n",
    "Evaluates semantic similarity between expected and actual output using an LLM judge.\n",
    "\n",
    "**Scoring:**\n",
    "- **Score range:** 0.0 to 1.0\n",
    "- **Pass threshold:** 0.9 (configurable per test case)\n",
    "- **Method:** Semantic similarity with penalty for omissions and contradictions\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1. Factual accuracy\n",
    "2. Completeness (no significant omissions)\n",
    "3. No contradictions with expected output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Format\n",
    "\n",
    "Results are stored in YAML format:\n",
    "\n",
    "```yaml\n",
    "results:\n",
    "  - model: claude-4-sonnet\n",
    "    coder: goose\n",
    "    case_name: PMID_28027860_Full_Text\n",
    "    case_group: Text extraction\n",
    "    metric_name: CorrectnessMetric\n",
    "    score: 0.92\n",
    "    passed: true\n",
    "    reason: \"Output matches expected with high semantic similarity\"\n",
    "    actual_output: \"...\"\n",
    "    expected_output: \"...\"\n",
    "    execution_time: 45.2\n",
    "    servers: [artl]\n",
    "    execution_metadata:\n",
    "      success: true\n",
    "      stdout: \"...\"\n",
    "      stderr: \"...\"\n",
    "\n",
    "summary:\n",
    "  total_cases: 100\n",
    "  passed: 47\n",
    "  failed: 53\n",
    "  pass_rate: 0.47\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility Checklist\n",
    "\n",
    "To reproduce the cross-agent comparison:\n",
    "\n",
    "- [ ] Install all coding agents (goose, claude, gemini)\n",
    "- [ ] Set up API keys in `~/anthropic.key`, `~/openai.key`, etc.\n",
    "- [ ] Install metacoder: `uv add metacoder`\n",
    "- [ ] Verify MCP servers are accessible: `uvx artl-mcp`, `uvx mcp-simple-pubmed`, etc.\n",
    "- [ ] Run Goose evaluation: `./run_goose_eval_fixed.sh`\n",
    "- [ ] Run Claude evaluation: `./run_claude_eval.sh`\n",
    "- [ ] Run Gemini evaluation: `./run_gemini_eval.sh`\n",
    "- [ ] Execute analysis notebook: `jupyter nbconvert --execute ...`\n",
    "- [ ] Review results in `notes/EXPERIMENT_1_RESULTS.md`\n",
    "- [ ] Check generated figures in `results/figures/`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known Limitations\n",
    "\n",
    "1. **Goose MCP Extension Compatibility:** pubmed-mcp and simple-pubmed fail with 100% execution errors\n",
    "2. **API Rate Limits:** Long-running evaluations may hit rate limits\n",
    "3. **Evaluation Time:** Each full run takes 2-3 hours\n",
    "4. **Non-determinism:** LLM responses may vary between runs\n",
    "5. **Cost:** Each run costs ~$10-20 in API fees (varies by model)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **Metacoder Framework:** https://github.com/Your-Org/metacoder\n",
    "- **ARTL MCP:** https://github.com/Your-Org/artl-mcp\n",
    "- **Simple PubMed MCP:** https://github.com/Your-Org/mcp-simple-pubmed\n",
    "- **BioMCP:** https://github.com/Your-Org/biomcp-python\n",
    "- **PubMed MCP:** https://github.com/chrismannina/pubmed-mcp\n",
    "- **Experiment Design:** `notes/experiment_1_cross_agent_comparison.md`\n",
    "- **Analysis Notebook:** `notebook/experiment_1_cross_agent_analysis.ipynb`\n",
    "- **Results:** `notes/EXPERIMENT_1_RESULTS.md`\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
