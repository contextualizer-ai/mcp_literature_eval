{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 1: Run Cross-Agent Evaluations\n\nThis notebook runs MCP evaluations across three coding agents: Claude Code, Goose, and Codex.\n\n**Each evaluation takes 2-3 hours** and tests 25 cases × 4 MCP servers = 100 evaluations per agent.\n\n**Total time: ~2-3 hours** (if run in parallel) or ~6-9 hours (if run sequentially)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\n### Environment Variables Required\n\n**CRITICAL: ALL agents require `OPENAI_API_KEY`** (DeepEval uses OpenAI for evaluation scoring)\n\n**Claude Code:**\n- `OPENAI_API_KEY`: From `~/openai.key` (for DeepEval CorrectnessMetric)\n- `ANTHROPIC_API_KEY`: From `~/cborg.key` (CBORG proxy)\n- `ANTHROPIC_BASE_URL`: `https://api.cborg.lbl.gov`\n- `PUBMED_EMAIL`, `PUBMED_API_KEY`: For MCP servers\n\n**Goose:**\n- `OPENAI_API_KEY`: From `~/openai.key` (for agent + DeepEval)\n- `PUBMED_EMAIL`, `PUBMED_API_KEY`: For MCP servers\n\n**Codex:**\n- `OPENAI_API_KEY`: From `~/openai.key` (for agent + DeepEval)\n- `PUBMED_EMAIL`, `PUBMED_API_KEY`: For MCP servers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "# Set working directory to project root\n",
    "project_root = Path.cwd().parent if 'notebook' in str(Path.cwd()) else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations in Parallel\n",
    "\n",
    "**Recommended:** Run all 3 agents simultaneously in separate terminals to complete in ~2-3 hours total.\n",
    "\n",
    "Open **3 terminal windows** and run these commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Terminal 1: Claude Code (via CBORG)\n\n```bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport ANTHROPIC_API_KEY=$(cat ~/cborg.key)\nexport ANTHROPIC_BASE_URL=https://api.cborg.lbl.gov\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/literature_mcp_eval_config_claude.yaml \\\n  -o results/compare_agents/claude_$(date +%Y%m%d).yaml\n```\n\n**Note:** Uses CBORG (LBL cluster proxy) for Anthropic API access. `OPENAI_API_KEY` is required for DeepEval's CorrectnessMetric (evaluation scorer)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal 2: Goose + gpt-4o (via OpenAI)\n",
    "\n",
    "```bash\n",
    "cd /Users/jtr4v/PythonProject/mcp_literature_eval\n",
    "export OPENAI_API_KEY=$(cat ~/openai.key)\n",
    "export PUBMED_EMAIL=justinreese@lbl.gov\n",
    "export PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\n",
    "uv run metacoder eval project/literature_mcp_eval_config_goose_gpt4o.yaml \\\n",
    "  -o results/compare_agents/goose_$(date +%Y%m%d).yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Terminal 3: Codex + gpt-4o (via OpenAI)\n\n```bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/literature_mcp_eval_config_codex.yaml \\\n  -o results/compare_agents/codex_$(date +%Y%m%d).yaml\n```\n\n**Note:** `OPENAI_API_KEY` is required for both the Codex agent and DeepEval's CorrectnessMetric (evaluation scorer)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Check Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from glob import glob\n\n# Find all result files\nresult_files = sorted(glob(\"results/compare_agents/*.yaml\"))\n\nprint(\"Experiment 1 Evaluation Results\")\nprint(\"=\" * 70)\n\nagents_found = {}\nfor f in result_files:\n    try:\n        # Get file size first to skip empty files\n        if os.path.getsize(f) == 0:\n            print(f\"\\n⚠️  {Path(f).name}: EMPTY FILE (evaluation incomplete or failed)\")\n            continue\n            \n        with open(f, 'r') as file:\n            data = yaml.safe_load(file)\n            if data and 'results' in data:\n                filename = Path(f).stem\n                # Extract agent name: agent_DATE.yaml\n                parts = filename.split('_')\n                agent = parts[0]\n                date = parts[1] if len(parts) > 1 else 'unknown'\n                \n                count = len(data['results'])\n                passed = sum(1 for r in data['results'] if r.get('passed', False))\n                pass_rate = (passed / count * 100) if count > 0 else 0\n                \n                agents_found[agent] = True\n                \n                print(f\"\\n{agent.upper()} ({date}):\")\n                print(f\"  File: {f}\")\n                print(f\"  Tests: {count}\")\n                print(f\"  Passed: {passed}\")\n                print(f\"  Pass rate: {pass_rate:.1f}%\")\n    except Exception as e:\n        print(f\"\\nError reading {f}: {e}\")\n\nprint(\"\\n\" + \"=\" * 70)\n\n# Check completeness\nexpected_agents = {'claude', 'goose', 'codex'}\nmissing_agents = expected_agents - set(agents_found.keys())\n\nif missing_agents:\n    print(f\"\\n⚠️  Missing evaluations for: {', '.join(missing_agents)}\")\nelse:\n    print(\"\\n✓ All agent evaluations complete!\")\n    print(\"\\nNext step: Run analysis in experiment_1_cross_agent_analysis.ipynb\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Configuration Details\n\n### Agents Tested\n\n1. **Claude Code** (claude-sonnet-4-20250514)\n   - Provider: Anthropic via CBORG proxy\n   - Config: `project/literature_mcp_eval_config_claude.yaml`\n\n2. **Goose** (gpt-4o)\n   - Provider: OpenAI\n   - Config: `project/literature_mcp_eval_config_goose_gpt4o.yaml`\n\n3. **Codex** (gpt-4o)\n   - Provider: OpenAI\n   - Config: `project/literature_mcp_eval_config_codex.yaml`\n\n### MCP Servers Tested\n\n1. **ARTL MCP** - Berkeley Lab Contextualizer AI\n   - Repo: https://github.com/contextualizer-ai/artl-mcp\n   - Command: `uvx artl-mcp`\n\n2. **Simple PubMed MCP**\n   - Repo: https://github.com/andybrandt/mcp-simple-pubmed\n   - Command: `uvx mcp-simple-pubmed`\n   - Env: `PUBMED_EMAIL=justinreese@lbl.gov`\n\n3. **BioMCP**\n   - Repo: https://github.com/genomoncology/biomcp\n   - Command: `uv run --with biomcp-python biomcp run`\n\n4. **PubMed MCP** (chrismannina)\n   - Repo: https://github.com/chrismannina/pubmed-mcp\n   - Command: `uv run --with git+https://github.com/chrismannina/pubmed-mcp@main -m src.main`\n   - Env: `PUBMED_API_KEY`, `PUBMED_EMAIL=justinreese@lbl.gov`\n\n### Test Cases\n\n25 literature retrieval tasks organized into groups:\n- **Text extraction** (11 cases)\n- **Metadata** (4 cases)\n- **Table/Figure/Legend extraction** (4 cases)\n- **Supplementary material** (3 cases)\n- **Summarization** (2 cases)\n- **Publication status** (1 case)\n\nSee [TEST_CASES.md](../TEST_CASES.md) for complete test case details.\n\n### Evaluation Framework\n\n- **Framework:** Metacoder (https://github.com/ai4curation/metacoder)\n- **Metric:** CorrectnessMetric (semantic similarity via DeepEval)\n- **Pass threshold:** 0.9 (90% semantic match)\n- **Total evaluations per agent:** 100 (25 cases × 4 MCPs)\n- **Duration:** ~2-3 hours per agent\n\n### CBORG Configuration (Claude Code)\n\nCBORG is LBL's cluster proxy for Anthropic API access:\n- **Base URL:** `https://api.cborg.lbl.gov`\n- **API Key:** Stored in `~/cborg.key`\n- **Credit tracking:** Monitor usage at CBORG dashboard\n\n**Note:** If CBORG credits run low, switch to direct Anthropic:\n```bash\nexport ANTHROPIC_API_KEY=$(cat ~/anthropic.key)\nunset ANTHROPIC_BASE_URL\n```\n\n### References\n\n- **MCP Server Catalog:** https://docs.google.com/spreadsheets/d/1506RuqfyUrBHd6lGNtY5j688CvVwk5mf5h_LEfsPIp4/edit?gid=614919216#gid=614919216\n- **Experiment Design:** `notes/EXPERIMENT_1_RESULTS.md`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}