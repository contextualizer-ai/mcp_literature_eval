{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Run Cross-Agent Evaluations\n",
    "\n",
    "This notebook runs MCP evaluations across three coding agents: Claude Code, Goose, and Gemini.\n",
    "\n",
    "**Each evaluation takes 2-3 hours** and tests 25 cases × 4 MCP servers = 100 evaluations per agent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Set working directory\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Claude Code Evaluation\n",
    "\n",
    "**Command:** `./run_claude_eval.sh`\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_agents/claude_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Claude evaluation\n",
    "# Uncomment to run:\n",
    "# !./run_claude_eval.sh\n",
    "\n",
    "print(\"Claude evaluation script: ./run_claude_eval.sh\")\n",
    "print(\"Status: Already completed (results/compare_agents/claude_20251031.yaml)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Goose Evaluation\n",
    "\n",
    "**Command:** `./run_goose_eval.sh`\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_agents/goose_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Goose evaluation\n",
    "# Uncomment to run:\n",
    "# !./run_goose_eval.sh\n",
    "\n",
    "print(\"Goose evaluation script: ./run_goose_eval.sh\")\n",
    "print(\"Status: Needs re-run with fixed MCP configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Gemini Evaluation\n",
    "\n",
    "**Command:** `./run_gemini_eval.sh`\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_agents/gemini_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Gemini evaluation\n",
    "# Uncomment to run:\n",
    "# !./run_gemini_eval.sh\n",
    "\n",
    "print(\"Gemini evaluation script: ./run_gemini_eval.sh\")\n",
    "print(\"Status: Currently running in background\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from glob import glob\n",
    "\n",
    "# Find all result files\n",
    "result_files = sorted(glob('results/compare_agents/*.yaml'))\n",
    "\n",
    "print(\"Available evaluation results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for f in result_files:\n",
    "    try:\n",
    "        with open(f, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            if data and 'results' in data:\n",
    "                agent = Path(f).stem.split('_')[0]\n",
    "                date = Path(f).stem.split('_')[1]\n",
    "                count = len(data['results'])\n",
    "                summary = data.get('summary', {})\n",
    "                pass_rate = summary.get('pass_rate', 0) * 100 if summary else 0\n",
    "                \n",
    "                print(f\"\\n{agent.upper()} ({date}):\")\n",
    "                print(f\"  File: {f}\")\n",
    "                print(f\"  Tests: {count}\")\n",
    "                print(f\"  Pass rate: {pass_rate:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError reading {f}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis\n",
    "\n",
    "Once all evaluations are complete, run the cross-agent analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-agent analysis notebook\n",
    "# Uncomment when all evaluations are complete:\n",
    "# !jupyter nbconvert --execute --to notebook --inplace experiment_1_cross_agent_analysis.ipynb\n",
    "\n",
    "print(\"Analysis notebook: experiment_1_cross_agent_analysis.ipynb\")\n",
    "print(\"Run after all three evaluations are complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration Details\n",
    "\n",
    "### MCP Servers Tested\n",
    "\n",
    "1. **ARTL MCP** - Berkeley Lab Contextualizer AI\n",
    "   - Repo: https://github.com/contextualizer-ai/artl-mcp\n",
    "   - Command: `uvx artl-mcp`\n",
    "\n",
    "2. **Simple PubMed MCP**\n",
    "   - Repo: https://github.com/andybrandt/mcp-simple-pubmed\n",
    "   - Command: `uvx mcp-simple-pubmed`\n",
    "   - Env: `PUBMED_EMAIL=justinreese@lbl.gov`\n",
    "\n",
    "3. **BioMCP**\n",
    "   - Repo: https://github.com/genomoncology/biomcp\n",
    "   - Command: `uv run --with biomcp-python biomcp run`\n",
    "\n",
    "4. **PubMed MCP** (chrismannina)\n",
    "   - Repo: https://github.com/chrismannina/pubmed-mcp\n",
    "   - Command: `uv run --with git+https://github.com/chrismannina/pubmed-mcp@main -m src.main`\n",
    "   - Env: `PUBMED_API_KEY`, `PUBMED_EMAIL=justinreese@lbl.gov`\n",
    "\n",
    "### Test Cases\n",
    "\n",
    "25 literature retrieval tasks organized into groups:\n",
    "- Text extraction (9 cases)\n",
    "- Metadata (8 cases)  \n",
    "- Table/Figure/Legend extraction (4 cases)\n",
    "- Summarization (2 cases)\n",
    "- Supplementary material (1 case)\n",
    "- Publication status (1 case)\n",
    "\n",
    "### Evaluation Framework\n",
    "\n",
    "- **Framework:** Metacoder (https://github.com/ai4curation/metacoder)\n",
    "- **Metric:** CorrectnessMetric (semantic similarity)\n",
    "- **Pass threshold:** 0.9\n",
    "- **Total evaluations per agent:** 100 (25 cases × 4 MCPs)\n",
    "\n",
    "### References\n",
    "\n",
    "- **MCP Server Catalog:** https://docs.google.com/spreadsheets/d/1506RuqfyUrBHd6lGNtY5j688CvVwk5mf5h_LEfsPIp4/edit?gid=614919216#gid=614919216\n",
    "- **Experiment Design:** `notes/experiment_1_cross_agent_comparison.md`\n",
    "- **Results Documentation:** `notes/EXPERIMENT_1_RESULTS.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
