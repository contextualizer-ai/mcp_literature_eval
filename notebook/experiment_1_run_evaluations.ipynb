{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Run Cross-Agent Evaluations\n",
    "\n",
    "This notebook runs MCP evaluations across three coding agents: Claude Code, Goose, and Gemini.\n",
    "\n",
    "**Each evaluation takes 2-3 hours** and tests 25 cases × 4 MCP servers = 100 evaluations per agent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Set working directory\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Claude Code Evaluation\n",
    "\n",
    "**Command:** `./run_claude_eval.sh`\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_agents/claude_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if Claude evaluation already exists\nclaude_result = f\"results/compare_agents/claude_{datetime.now().strftime('%Y%m%d')}.yaml\"\nif os.path.exists(claude_result):\n    print(f\"✓ Claude evaluation already completed: {claude_result}\")\nelse:\n    print(\"Running Claude evaluation (this takes 2-3 hours)...\")\n    result = subprocess.run([\"./run_claude_eval.sh\"], capture_output=True, text=True)\n    if result.returncode == 0:\n        print(\"✓ Claude evaluation complete!\")\n    else:\n        print(f\"✗ Error: {result.stderr}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Goose Evaluation\n",
    "\n",
    "**Command:** `./run_goose_eval.sh`\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_agents/goose_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if Goose evaluation already exists\ngoose_result = f\"results/compare_agents/goose_{datetime.now().strftime('%Y%m%d')}.yaml\"\nif os.path.exists(goose_result):\n    print(f\"✓ Goose evaluation already completed: {goose_result}\")\nelse:\n    print(\"Running Goose evaluation (this takes 2-3 hours)...\")\n    result = subprocess.run([\"./run_goose_eval.sh\"], capture_output=True, text=True)\n    if result.returncode == 0:\n        print(\"✓ Goose evaluation complete!\")\n    else:\n        print(f\"✗ Error: {result.stderr}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Gemini Evaluation\n",
    "\n",
    "**Command:** `./run_gemini_eval.sh`\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_agents/gemini_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if Gemini evaluation already exists\ngemini_result = f\"results/compare_agents/gemini_{datetime.now().strftime('%Y%m%d')}.yaml\"\nif os.path.exists(gemini_result):\n    print(f\"✓ Gemini evaluation already completed: {gemini_result}\")\nelse:\n    print(\"Running Gemini evaluation (this takes 2-3 hours)...\")\n    result = subprocess.run([\"./run_gemini_eval.sh\"], capture_output=True, text=True)\n    if result.returncode == 0:\n        print(\"✓ Gemini evaluation complete!\")\n    else:\n        print(f\"✗ Error: {result.stderr}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from glob import glob\n",
    "\n",
    "# Find all result files\n",
    "result_files = sorted(glob('results/compare_agents/*.yaml'))\n",
    "\n",
    "print(\"Available evaluation results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for f in result_files:\n",
    "    try:\n",
    "        with open(f, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            if data and 'results' in data:\n",
    "                agent = Path(f).stem.split('_')[0]\n",
    "                date = Path(f).stem.split('_')[1]\n",
    "                count = len(data['results'])\n",
    "                summary = data.get('summary', {})\n",
    "                pass_rate = summary.get('pass_rate', 0) * 100 if summary else 0\n",
    "                \n",
    "                print(f\"\\n{agent.upper()} ({date}):\")\n",
    "                print(f\"  File: {f}\")\n",
    "                print(f\"  Tests: {count}\")\n",
    "                print(f\"  Pass rate: {pass_rate:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError reading {f}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis\n",
    "\n",
    "Once all evaluations are complete, run the cross-agent analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if all evaluations are complete, then run analysis\nresult_files = glob('results/compare_agents/*.yaml')\nagents_complete = {'claude': False, 'goose': False, 'gemini': False}\n\nfor f in result_files:\n    agent = Path(f).stem.split('_')[0]\n    if agent in agents_complete:\n        agents_complete[agent] = True\n\nif all(agents_complete.values()):\n    print(\"All evaluations complete! Running cross-agent analysis...\")\n    result = subprocess.run([\n        \"jupyter\", \"nbconvert\", \"--execute\", \"--to\", \"notebook\", \n        \"--inplace\", \"experiment_1_cross_agent_analysis.ipynb\"\n    ], capture_output=True, text=True)\n    if result.returncode == 0:\n        print(\"✓ Analysis complete!\")\n    else:\n        print(f\"✗ Error: {result.stderr}\")\nelse:\n    missing = [k for k, v in agents_complete.items() if not v]\n    print(f\"⏳ Waiting for evaluations to complete: {', '.join(missing)}\")\n    print(\"Run this cell again after all evaluations finish.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Configuration Details\n",
    "\n",
    "### MCP Servers Tested\n",
    "\n",
    "1. **ARTL MCP** - Berkeley Lab Contextualizer AI\n",
    "   - Repo: https://github.com/contextualizer-ai/artl-mcp\n",
    "   - Command: `uvx artl-mcp`\n",
    "\n",
    "2. **Simple PubMed MCP**\n",
    "   - Repo: https://github.com/andybrandt/mcp-simple-pubmed\n",
    "   - Command: `uvx mcp-simple-pubmed`\n",
    "   - Env: `PUBMED_EMAIL=justinreese@lbl.gov`\n",
    "\n",
    "3. **BioMCP**\n",
    "   - Repo: https://github.com/genomoncology/biomcp\n",
    "   - Command: `uv run --with biomcp-python biomcp run`\n",
    "\n",
    "4. **PubMed MCP** (chrismannina)\n",
    "   - Repo: https://github.com/chrismannina/pubmed-mcp\n",
    "   - Command: `uv run --with git+https://github.com/chrismannina/pubmed-mcp@main -m src.main`\n",
    "   - Env: `PUBMED_API_KEY`, `PUBMED_EMAIL=justinreese@lbl.gov`\n",
    "\n",
    "### Test Cases\n",
    "\n",
    "25 literature retrieval tasks organized into groups:\n",
    "- Text extraction (9 cases)\n",
    "- Metadata (8 cases)  \n",
    "- Table/Figure/Legend extraction (4 cases)\n",
    "- Summarization (2 cases)\n",
    "- Supplementary material (1 case)\n",
    "- Publication status (1 case)\n",
    "\n",
    "### Evaluation Framework\n",
    "\n",
    "- **Framework:** Metacoder (https://github.com/ai4curation/metacoder)\n",
    "- **Metric:** CorrectnessMetric (semantic similarity)\n",
    "- **Pass threshold:** 0.9\n",
    "- **Total evaluations per agent:** 100 (25 cases × 4 MCPs)\n",
    "\n",
    "### References\n",
    "\n",
    "- **MCP Server Catalog:** https://docs.google.com/spreadsheets/d/1506RuqfyUrBHd6lGNtY5j688CvVwk5mf5h_LEfsPIp4/edit?gid=614919216#gid=614919216\n",
    "- **Experiment Design:** `notes/experiment_1_cross_agent_comparison.md`\n",
    "- **Results Documentation:** `notes/EXPERIMENT_1_RESULTS.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}