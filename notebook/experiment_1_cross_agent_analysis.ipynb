{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Cross-Agent Comparison Analysis\n",
    "\n",
    "This notebook analyzes MCP performance across different coding agents (goose-cli, claude-code, gemini-cli).\n",
    "\n",
    "**Objective:** Determine whether agent choice affects MCP retrieval performance.\n",
    "\n",
    "**See:** `notes/experiment_1_cross_agent_comparison.md` for detailed experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T20:56:38.380206Z",
     "iopub.status.busy": "2025-11-06T20:56:38.380111Z",
     "iopub.status.idle": "2025-11-06T20:56:45.622742Z",
     "shell.execute_reply": "2025-11-06T20:56:45.622304Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results from Different Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T20:56:45.624622Z",
     "iopub.status.busy": "2025-11-06T20:56:45.624473Z",
     "iopub.status.idle": "2025-11-06T20:56:56.927706Z",
     "shell.execute_reply": "2025-11-06T20:56:56.927421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded results for claude: ../results/compare_agents/claude_20251106.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded results for gemini: ../results/compare_agents/gemini_20251106.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded results for goose: ../results/compare_agents/goose_20251106.yaml\n",
      "\n",
      "3 agent(s) loaded: ['claude', 'gemini', 'goose']\n"
     ]
    }
   ],
   "source": [
    "# Define result files for each agent (November 6, 2025 run)\n",
    "result_files = {\n",
    "    \"claude\": \"../results/compare_agents/claude_20251106.yaml\",  # Claude Code results\n",
    "    \"gemini\": \"../results/compare_agents/gemini_20251106.yaml\",  # Gemini CLI results\n",
    "    \"goose\": \"../results/compare_agents/goose_20251106.yaml\",  # Goose CLI results\n",
    "}\n",
    "\n",
    "# Load results\n",
    "agent_results = {}\n",
    "for agent, filepath in result_files.items():\n",
    "    if Path(filepath).exists():\n",
    "        with open(filepath, \"r\") as f:\n",
    "            agent_results[agent] = yaml.safe_load(f)\n",
    "        print(f\"âœ“ Loaded results for {agent}: {filepath}\")\n",
    "    else:\n",
    "        print(f\"âœ— Results not found for {agent}: {filepath}\")\n",
    "\n",
    "print(f\"\\n{len(agent_results)} agent(s) loaded: {list(agent_results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T20:56:56.929267Z",
     "iopub.status.busy": "2025-11-06T20:56:56.929168Z",
     "iopub.status.idle": "2025-11-06T20:56:56.945053Z",
     "shell.execute_reply": "2025-11-06T20:56:56.944769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (300, 16)\n",
      "\n",
      "Agents: ['claude' 'gemini' 'goose']\n",
      "MCPs: ['artl' 'simple-pubmed' 'biomcp' 'pubmed-mcp']\n",
      "Case groups: ['Text extraction' 'Metadata' 'Summarization'\n",
      " 'Table / Figure / Figure Legend extraction' 'Supplementary material'\n",
      " 'Publication status']\n"
     ]
    }
   ],
   "source": [
    "# Convert each agent's results to DataFrame\n",
    "dfs = {}\n",
    "for agent, results in agent_results.items():\n",
    "    df = pd.DataFrame(results[\"results\"])\n",
    "    df = df.explode(\"servers\")  # Expand so each server gets its own row\n",
    "    df[\"MCP\"] = df[\"servers\"]\n",
    "    df[\"agent\"] = agent  # Add agent identifier\n",
    "    dfs[agent] = df\n",
    "\n",
    "# Combine all results into single DataFrame\n",
    "if dfs:\n",
    "    df_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "    print(f\"\\nAgents: {df_combined['agent'].unique()}\")\n",
    "    print(f\"MCPs: {df_combined['MCP'].unique()}\")\n",
    "    print(f\"Case groups: {df_combined['case_group'].unique()}\")\n",
    "else:\n",
    "    print(\"No results loaded yet. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Figure 1: MCP Performance Across Coding Agents\n\n4 MCPs Ã— 3 agents (goose-cli, claude-code, gemini-cli) - Overall pass rate for each combination"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T20:56:56.946466Z",
     "iopub.status.busy": "2025-11-06T20:56:56.946317Z",
     "iopub.status.idle": "2025-11-06T20:56:57.323205Z",
     "shell.execute_reply": "2025-11-06T20:56:57.322794Z"
    }
   },
   "outputs": [],
   "source": "if \"df_combined\" in locals():\n    # Create violin plot for score distributions by agent and MCP\n    fig, ax = plt.subplots(figsize=(16, 7))\n\n    mcps = sorted(df_combined[\"MCP\"].unique())\n    agents = sorted(df_combined[\"agent\"].unique())\n    \n    colors = {'claude': '#1f77b4', 'gemini': '#ff7f0e', 'goose': '#2ca02c'}\n    \n    # Prepare data for violin plots\n    positions = []\n    data_to_plot = []\n    labels = []\n    violin_colors = []\n    \n    pos = 0\n    for mcp_idx, mcp in enumerate(mcps):\n        for agent_idx, agent in enumerate(agents):\n            agent_mcp_data = df_combined[(df_combined[\"agent\"] == agent) & \n                                         (df_combined[\"MCP\"] == mcp)][\"score\"].dropna()\n            if len(agent_mcp_data) > 0:\n                data_to_plot.append(agent_mcp_data)\n                positions.append(pos)\n                labels.append(f\"{agent}\")\n                violin_colors.append(colors.get(agent, '#333333'))\n                pos += 1\n        pos += 1  # Add gap between MCPs\n    \n    # Create violin plots\n    parts = ax.violinplot(data_to_plot, positions=positions, widths=0.6,\n                          showmeans=True, showmedians=True, showextrema=True)\n    \n    # Color the violins\n    for i, pc in enumerate(parts['bodies']):\n        pc.set_facecolor(violin_colors[i])\n        pc.set_alpha(0.6)\n    \n    # Customize violin plot elements\n    for partname in ('cbars', 'cmins', 'cmaxes', 'cmedians', 'cmeans'):\n        if partname in parts:\n            parts[partname].set_edgecolor('black')\n            parts[partname].set_linewidth(1)\n    \n    # Overlay strip plot with individual points\n    pos = 0\n    for mcp_idx, mcp in enumerate(mcps):\n        for agent_idx, agent in enumerate(agents):\n            agent_mcp_data = df_combined[(df_combined[\"agent\"] == agent) & \n                                         (df_combined[\"MCP\"] == mcp)][\"score\"].dropna()\n            if len(agent_mcp_data) > 0:\n                # Add jitter to x position for better visibility\n                x_jitter = np.random.normal(pos, 0.08, size=len(agent_mcp_data))\n                ax.scatter(x_jitter, agent_mcp_data, \n                          color=colors.get(agent, '#333333'),\n                          alpha=0.4, s=20, zorder=3, edgecolors='white', linewidths=0.5)\n                pos += 1\n        pos += 1\n    \n    # Add horizontal line for pass threshold\n    ax.axhline(y=0.9, color='red', linestyle='--', linewidth=2, alpha=0.6, label='Pass threshold (0.9)')\n    \n    # Set x-axis labels\n    mcp_positions = []\n    for mcp_idx, mcp in enumerate(mcps):\n        mcp_center = mcp_idx * (len(agents) + 1) + len(agents) / 2 - 0.5\n        mcp_positions.append(mcp_center)\n    \n    ax.set_xticks(mcp_positions)\n    ax.set_xticklabels(mcps, fontsize=11)\n    ax.set_ylabel(\"Semantic Similarity Score\", fontsize=12, fontweight='bold')\n    ax.set_xlabel(\"MCP Server\", fontsize=12, fontweight='bold')\n    ax.set_title(\"Figure 1: MCP Performance Across Coding Agents\", fontsize=14, fontweight=\"bold\", pad=20)\n    ax.set_ylim(-0.05, 1.05)\n    ax.grid(axis=\"y\", alpha=0.3)\n    \n    # Create custom legend\n    from matplotlib.patches import Patch\n    legend_elements = [Patch(facecolor=colors[agent], alpha=0.6, label=agent) \n                      for agent in agents]\n    legend_elements.append(plt.Line2D([0], [0], color='red', linestyle='--', linewidth=2, \n                                     label='Pass threshold'))\n    ax.legend(handles=legend_elements, title=\"Agent\", title_fontsize=11, fontsize=10, loc='lower right')\n    \n    plt.tight_layout()\n    plt.savefig(\n        \"../results/figures/fig1_mcp_performance_by_agent.png\",\n        dpi=300,\n        bbox_inches=\"tight\",\n    )\n    plt.show()\n\n    # Print summary statistics\n    print(\"\\nFigure 1 Summary - Score Statistics by MCP and Agent:\")\n    for mcp in mcps:\n        print(f\"\\n{mcp}:\")\n        for agent in agents:\n            agent_mcp_scores = df_combined[(df_combined[\"agent\"] == agent) & \n                                           (df_combined[\"MCP\"] == mcp)][\"score\"]\n            median = agent_mcp_scores.median()\n            pass_rate = (agent_mcp_scores >= 0.9).sum() / len(agent_mcp_scores) * 100\n            print(f\"  {agent:10s}: median={median:.3f}, pass_rate={pass_rate:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Figure 2: Performance by Evaluation Type\n\nBreakdown by test categories comparing all 4 MCPs within each category"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T20:56:57.324647Z",
     "iopub.status.busy": "2025-11-06T20:56:57.324552Z",
     "iopub.status.idle": "2025-11-06T20:56:57.584028Z",
     "shell.execute_reply": "2025-11-06T20:56:57.583540Z"
    }
   },
   "outputs": [],
   "source": "if \"df_combined\" in locals():\n    # Calculate % passed and counts by case_group and MCP (across all agents)\n    category_pass_rates = (\n        df_combined.groupby([\"case_group\", \"MCP\"])[\"passed\"]\n        .agg(['mean', 'sum', 'count'])\n        .reset_index()\n    )\n    category_pass_rates[\"percent_passed\"] = category_pass_rates[\"mean\"] * 100\n\n    # Pivot for heatmap\n    heatmap_data = category_pass_rates.pivot(\n        index=\"case_group\", columns=\"MCP\", values=\"percent_passed\"\n    )\n    \n    # Create count data for annotations\n    count_data = category_pass_rates.copy()\n    count_data[\"count_str\"] = count_data.apply(\n        lambda x: f\"{int(x['sum'])}/{int(x['count'])}\", axis=1\n    )\n    count_pivot = count_data.pivot(\n        index=\"case_group\", columns=\"MCP\", values=\"count_str\"\n    )\n\n    # Create heatmap with annotations\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    # Create annotations combining percentage and counts\n    annot_data = heatmap_data.copy()\n    for i, case_group in enumerate(annot_data.index):\n        for j, mcp in enumerate(annot_data.columns):\n            pct = heatmap_data.loc[case_group, mcp]\n            count_str = count_pivot.loc[case_group, mcp]\n            annot_data.loc[case_group, mcp] = f\"{pct:.1f}%\\n{count_str}\"\n    \n    sns.heatmap(\n        heatmap_data,\n        annot=annot_data,\n        fmt=\"\",\n        cmap=\"RdYlGn\",\n        vmin=0,\n        vmax=100,\n        cbar_kws={\"label\": \"% Passed\"},\n        linewidths=0.5,\n        ax=ax,\n        annot_kws={\"fontsize\": 9}\n    )\n    \n    ax.set_title(\"Figure 2: Performance by Evaluation Type\", fontsize=14, fontweight=\"bold\", pad=20)\n    ax.set_xlabel(\"MCP Server\", fontsize=12, fontweight='bold')\n    ax.set_ylabel(\"Evaluation Category\", fontsize=12, fontweight='bold')\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n\n    plt.tight_layout()\n    plt.savefig(\n        \"../results/figures/fig2_performance_by_category.png\", dpi=300, bbox_inches=\"tight\"\n    )\n    plt.show()\n\n    print(\"\\nFigure 2 Summary - Pass Rates by Category and MCP:\")\n    print(\"\\nPercentages:\")\n    print(heatmap_data.round(1))\n    print(\"\\nCounts (passed/total):\")\n    print(count_pivot)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Figure 3: Model Comparison\n\nSame agent (goose-cli) tested with different models (gpt-4o, gpt-4o-mini, gpt-5) - How model choice affects MCP performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T20:56:57.585765Z",
     "iopub.status.busy": "2025-11-06T20:56:57.585650Z",
     "iopub.status.idle": "2025-11-06T20:56:57.897521Z",
     "shell.execute_reply": "2025-11-06T20:56:57.897027Z"
    }
   },
   "outputs": [],
   "source": "# Load model comparison data (goose-cli with different models)\nmodel_files = {\n    \"gpt-4o-mini\": \"../results/compare_models/goose_gpt4o_mini_20251105.yaml\",\n    \"gpt-4o\": \"../results/compare_models/goose_gpt4o_20251104.yaml\",\n    \"gpt-5\": \"../results/compare_models/goose_gpt5_20251104.yaml\",\n}\n\n# Load model results\nmodel_results = {}\nfor model, filepath in model_files.items():\n    if Path(filepath).exists():\n        with open(filepath, \"r\") as f:\n            model_results[model] = yaml.safe_load(f)\n        print(f\"âœ“ Loaded {model} results\")\n    else:\n        print(f\"âœ— {model} results not found\")\n\n# Convert to DataFrames\nmodel_dfs = {}\nfor model, results in model_results.items():\n    df = pd.DataFrame(results[\"results\"])\n    df = df.explode(\"servers\")\n    df[\"MCP\"] = df[\"servers\"]\n    df[\"model\"] = model\n    model_dfs[model] = df\n\n# Combine model comparison data\nif model_dfs:\n    df_models = pd.concat(model_dfs.values(), ignore_index=True)\n    \n    # Create violin plot with strip overlay for score distributions by model and MCP\n    fig, ax = plt.subplots(figsize=(16, 7))\n\n    mcps = sorted(df_models[\"MCP\"].unique())\n    models = [\"gpt-4o-mini\", \"gpt-4o\", \"gpt-5\"]  # Order by capability\n    \n    # Blue gradient: light â†’ medium â†’ dark\n    colors = {\n        'gpt-4o-mini': '#a6cee3',  # light blue\n        'gpt-4o': '#2b8cbe',        # medium blue\n        'gpt-5': '#08519c'          # dark blue\n    }\n    \n    # Prepare data for violin plots\n    positions = []\n    data_to_plot = []\n    violin_colors = []\n    \n    pos = 0\n    for mcp_idx, mcp in enumerate(mcps):\n        for model_idx, model in enumerate(models):\n            model_mcp_data = df_models[(df_models[\"model\"] == model) & \n                                       (df_models[\"MCP\"] == mcp)][\"score\"].dropna()\n            if len(model_mcp_data) > 0:\n                data_to_plot.append(model_mcp_data)\n                positions.append(pos)\n                violin_colors.append(colors.get(model, '#34495e'))\n                pos += 1\n        pos += 1  # Add gap between MCPs\n    \n    # Create violin plots\n    parts = ax.violinplot(data_to_plot, positions=positions, widths=0.6,\n                          showmeans=True, showmedians=True, showextrema=True)\n    \n    # Color the violins\n    for i, pc in enumerate(parts['bodies']):\n        pc.set_facecolor(violin_colors[i])\n        pc.set_alpha(0.6)\n    \n    # Customize violin plot elements\n    for partname in ('cbars', 'cmins', 'cmaxes', 'cmedians', 'cmeans'):\n        if partname in parts:\n            parts[partname].set_edgecolor('black')\n            parts[partname].set_linewidth(1)\n    \n    # Overlay strip plot with individual points\n    pos = 0\n    for mcp_idx, mcp in enumerate(mcps):\n        for model_idx, model in enumerate(models):\n            model_mcp_data = df_models[(df_models[\"model\"] == model) & \n                                       (df_models[\"MCP\"] == mcp)][\"score\"].dropna()\n            if len(model_mcp_data) > 0:\n                # Add jitter to x position for better visibility\n                x_jitter = np.random.normal(pos, 0.08, size=len(model_mcp_data))\n                ax.scatter(x_jitter, model_mcp_data, \n                          color=colors.get(model, '#34495e'),\n                          alpha=0.4, s=20, zorder=3, edgecolors='white', linewidths=0.5)\n                pos += 1\n        pos += 1\n    \n    # Add horizontal line for pass threshold\n    ax.axhline(y=0.9, color='red', linestyle='--', linewidth=2, alpha=0.6, label='Pass threshold (0.9)')\n    \n    # Set x-axis labels\n    mcp_positions = []\n    for mcp_idx, mcp in enumerate(mcps):\n        mcp_center = mcp_idx * (len(models) + 1) + len(models) / 2 - 0.5\n        mcp_positions.append(mcp_center)\n    \n    ax.set_xticks(mcp_positions)\n    ax.set_xticklabels(mcps, fontsize=11)\n    ax.set_ylabel(\"Semantic Similarity Score\", fontsize=12, fontweight='bold')\n    ax.set_xlabel(\"MCP Server\", fontsize=12, fontweight='bold')\n    ax.set_title(\"Figure 3: Model Comparison (goose-cli)\", fontsize=14, fontweight=\"bold\", pad=20)\n    ax.set_ylim(-0.05, 1.05)\n    ax.grid(axis=\"y\", alpha=0.3)\n    \n    # Create custom legend\n    from matplotlib.patches import Patch\n    legend_elements = [Patch(facecolor=colors[model], alpha=0.6, label=model) \n                      for model in models]\n    legend_elements.append(plt.Line2D([0], [0], color='red', linestyle='--', linewidth=2, \n                                     label='Pass threshold'))\n    ax.legend(handles=legend_elements, title=\"Model\", title_fontsize=11, fontsize=10, loc='lower right')\n    \n    plt.tight_layout()\n    plt.savefig(\n        \"../results/figures/fig3_model_comparison.png\",\n        dpi=300,\n        bbox_inches=\"tight\",\n    )\n    plt.show()\n\n    # Print summary statistics\n    print(\"\\nFigure 3 Summary - Score Statistics by Model and MCP:\")\n    for mcp in mcps:\n        print(f\"\\n{mcp}:\")\n        for model in models:\n            model_mcp_scores = df_models[(df_models[\"model\"] == model) & \n                                         (df_models[\"MCP\"] == mcp)][\"score\"]\n            if len(model_mcp_scores) > 0:\n                median = model_mcp_scores.median()\n                pass_rate = (model_mcp_scores >= 0.9).sum() / len(model_mcp_scores) * 100\n                print(f\"  {model:15s}: median={median:.3f}, pass_rate={pass_rate:.1f}%\")\n    \n    # Overall performance by model\n    print(\"\\nOverall Performance by Model:\")\n    for model in models:\n        model_scores = df_models[df_models[\"model\"] == model][\"score\"]\n        median = model_scores.median()\n        pass_rate = (model_scores >= 0.9).sum() / len(model_scores) * 100\n        print(f\"  {model:15s}: median={median:.3f}, pass_rate={pass_rate:.1f}%\")\nelse:\n    print(\"No model comparison data loaded.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary of Key Findings\n\nThree main findings from the cross-agent and model comparison analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T20:56:58.128804Z",
     "iopub.status.busy": "2025-11-06T20:56:58.128711Z",
     "iopub.status.idle": "2025-11-06T20:56:58.133161Z",
     "shell.execute_reply": "2025-11-06T20:56:58.132870Z"
    }
   },
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"KEY FINDINGS: Experiment 1 - Cross-Agent and Model Comparison\")\nprint(\"=\" * 80)\n\nif \"df_combined\" in locals():\n    print(\"\\nðŸ“Š FIGURE 1: MCP Performance Across Coding Agents\")\n    print(\"-\" * 80)\n    overall_pass_rates = df_combined.groupby(\"agent\")[\"passed\"].mean() * 100\n    print(\"\\nOverall Pass Rates by Agent:\")\n    for agent in ['claude', 'goose', 'gemini']:\n        if agent in overall_pass_rates.index:\n            print(f\"   â€¢ {agent:12s}: {overall_pass_rates[agent]:5.1f}%\")\n    \n    max_diff = overall_pass_rates.max() - overall_pass_rates.min()\n    print(f\"\\n   â†’ Agent choice significantly affects performance ({max_diff:.1f} pp difference)\")\n    print(f\"   â†’ Claude Code outperforms Gemini CLI and Goose CLI\")\n\nif \"category_pass_rates\" in locals():\n    print(\"\\n\\nðŸ“Š FIGURE 2: Performance by Evaluation Type\")\n    print(\"-\" * 80)\n    # Find best and worst performing categories\n    category_avg = category_pass_rates.groupby(\"case_group\")[\"percent_passed\"].mean().sort_values(ascending=False)\n    print(\"\\nCategory Performance (averaged across all MCPs):\")\n    for cat, rate in category_avg.items():\n        print(f\"   â€¢ {cat:45s}: {rate:5.1f}%\")\n    \n    print(f\"\\n   â†’ Best performing: {category_avg.index[0]}\")\n    print(f\"   â†’ Most challenging: {category_avg.index[-1]}\")\n\nif \"df_models\" in locals():\n    print(\"\\n\\nðŸ“Š FIGURE 3: Model Comparison\")\n    print(\"-\" * 80)\n    overall_model_rates = df_models.groupby(\"model\")[\"passed\"].mean() * 100\n    print(\"\\nOverall Pass Rates by Model (goose-cli):\")\n    for model in ['gpt-4o', 'gpt-4o-mini', 'gpt-5']:\n        if model in overall_model_rates.index:\n            print(f\"   â€¢ {model:12s}: {overall_model_rates[model]:5.1f}%\")\n    \n    if len(overall_model_rates) > 1:\n        model_diff = overall_model_rates.max() - overall_model_rates.min()\n        best_model = overall_model_rates.idxmax()\n        print(f\"\\n   â†’ Model choice affects performance ({model_diff:.1f} pp difference)\")\n        print(f\"   â†’ Best performing model: {best_model}\")\n\nprint(\"\\n\" + \"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nBased on these findings:\n\n1. **Agent selection matters**: Claude Code shows significantly better MCP retrieval performance\n2. **Category-specific analysis**: Identify why certain evaluation types are more challenging\n3. **Model optimization**: Investigate if newer/larger models improve performance consistently\n4. **Integration with manuscript**: Export these three figures for publication\n\nSee `notes/experiment_1_cross_agent_comparison.md` for detailed experimental design and methodology."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}