{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Cross-Agent Comparison Analysis\n",
    "\n",
    "This notebook analyzes MCP performance across different coding agents (goose-cli, claude-code, gemini-cli).\n",
    "\n",
    "**Objective:** Determine whether agent choice affects MCP retrieval performance.\n",
    "\n",
    "**See:** `notes/experiment_1_cross_agent_comparison.md` for detailed experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results from Different Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define result files for each agent\n",
    "result_files = {\n",
    "    'goose': '../results/mcp_literature_eval_results_20250917.yaml',  # Baseline\n",
    "    'claude-code': '../results/mcp_literature_eval_results_claude_YYYYMMDD.yaml',  # To be generated\n",
    "    'gemini': '../results/mcp_literature_eval_results_gemini_YYYYMMDD.yaml',  # To be generated (optional)\n",
    "}\n",
    "\n",
    "# Load results\n",
    "agent_results = {}\n",
    "for agent, filepath in result_files.items():\n",
    "    if Path(filepath).exists():\n",
    "        with open(filepath, 'r') as f:\n",
    "            agent_results[agent] = yaml.safe_load(f)\n",
    "        print(f\"✓ Loaded results for {agent}\")\n",
    "    else:\n",
    "        print(f\"✗ Results not found for {agent}: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each agent's results to DataFrame\n",
    "dfs = {}\n",
    "for agent, results in agent_results.items():\n",
    "    df = pd.DataFrame(results['results'])\n",
    "    df = df.explode('servers')  # Expand so each server gets its own row\n",
    "    df['MCP'] = df['servers']\n",
    "    df['agent'] = agent  # Add agent identifier\n",
    "    dfs[agent] = df\n",
    "\n",
    "# Combine all results into single DataFrame\n",
    "if dfs:\n",
    "    df_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "    print(f\"\\nAgents: {df_combined['agent'].unique()}\")\n",
    "    print(f\"MCPs: {df_combined['MCP'].unique()}\")\n",
    "    print(f\"Case groups: {df_combined['case_group'].unique()}\")\n",
    "else:\n",
    "    print(\"No results loaded yet. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Performance by Agent\n",
    "\n",
    "Compare pass rates across agents for each MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_combined' in locals():\n",
    "    # Calculate % passed by MCP and agent\n",
    "    pass_rates = df_combined.groupby(['agent', 'MCP'])['passed'].mean().reset_index()\n",
    "    pass_rates['percent_passed'] = pass_rates['passed'] * 100\n",
    "\n",
    "    # Create grouped bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    mcps = pass_rates['MCP'].unique()\n",
    "    agents = pass_rates['agent'].unique()\n",
    "    x = np.arange(len(mcps))\n",
    "    width = 0.8 / len(agents)\n",
    "    \n",
    "    for i, agent in enumerate(agents):\n",
    "        agent_data = pass_rates[pass_rates['agent'] == agent]\n",
    "        heights = [agent_data[agent_data['MCP'] == mcp]['percent_passed'].values[0] if len(agent_data[agent_data['MCP'] == mcp]) > 0 else 0 for mcp in mcps]\n",
    "        bars = ax.bar(x + i * width, heights, width, label=agent, alpha=0.8)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                       f'{height:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('MCP Server', fontsize=12)\n",
    "    ax.set_ylabel('% Passed', fontsize=12)\n",
    "    ax.set_title('MCP Performance by Agent', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x + width * (len(agents) - 1) / 2)\n",
    "    ax.set_xticklabels(mcps)\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.legend(title='Agent')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/exp1_overall_performance_by_agent.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nPass Rates by MCP and Agent:\")\n",
    "    print(pass_rates.pivot(index='MCP', columns='agent', values='percent_passed').round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Case Group Performance Heatmap\n",
    "\n",
    "Identify which task types are agent-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_combined' in locals():\n",
    "    # Calculate % passed by case_group and agent\n",
    "    group_pass_rates = df_combined.groupby(['case_group', 'agent'])['passed'].mean().reset_index()\n",
    "    group_pass_rates['percent_passed'] = group_pass_rates['passed'] * 100\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    heatmap_data = group_pass_rates.pivot(index='case_group', columns='agent', values='percent_passed')\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                vmin=0, vmax=100, cbar_kws={'label': '% Passed'},\n",
    "                linewidths=0.5, ax=ax)\n",
    "    ax.set_title('Task Performance by Agent (% Passed)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Agent', fontsize=12)\n",
    "    ax.set_ylabel('Case Group', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/exp1_case_group_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCase Group Performance:\")\n",
    "    print(heatmap_data.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Score Distribution Comparison\n",
    "\n",
    "Compare semantic similarity score distributions across agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_combined' in locals():\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Create violin plots\n",
    "    parts = ax.violinplot([df_combined[df_combined['agent'] == agent]['score'].dropna() \n",
    "                           for agent in df_combined['agent'].unique()],\n",
    "                          positions=range(len(df_combined['agent'].unique())),\n",
    "                          showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Overlay box plots\n",
    "    bp = ax.boxplot([df_combined[df_combined['agent'] == agent]['score'].dropna() \n",
    "                     for agent in df_combined['agent'].unique()],\n",
    "                    positions=range(len(df_combined['agent'].unique())),\n",
    "                    widths=0.1, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue', alpha=0.5),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "    \n",
    "    ax.set_xticks(range(len(df_combined['agent'].unique())))\n",
    "    ax.set_xticklabels(df_combined['agent'].unique())\n",
    "    ax.set_ylabel('Semantic Similarity Score', fontsize=12)\n",
    "    ax.set_xlabel('Agent', fontsize=12)\n",
    "    ax.set_title('Score Distribution by Agent', fontsize=14, fontweight='bold')\n",
    "    ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='Pass threshold (0.9)')\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/exp1_score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nScore Statistics by Agent:\")\n",
    "    print(df_combined.groupby('agent')['score'].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Rate Analysis\n",
    "\n",
    "Compare error rates and execution stability across agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_combined' in locals():\n",
    "    # Calculate error rates (assuming 'error' column exists or we infer from missing scores)\n",
    "    error_analysis = df_combined.groupby('agent').agg({\n",
    "        'passed': ['sum', 'count'],\n",
    "        'score': lambda x: x.isna().sum()  # Count missing scores as errors\n",
    "    }).round(2)\n",
    "    \n",
    "    error_analysis.columns = ['passed_count', 'total_count', 'error_count']\n",
    "    error_analysis['failed_count'] = error_analysis['total_count'] - error_analysis['passed_count'] - error_analysis['error_count']\n",
    "    error_analysis['pass_rate'] = (error_analysis['passed_count'] / error_analysis['total_count'] * 100).round(1)\n",
    "    error_analysis['error_rate'] = (error_analysis['error_count'] / error_analysis['total_count'] * 100).round(1)\n",
    "    \n",
    "    print(\"\\nExecution Summary by Agent:\")\n",
    "    print(error_analysis)\n",
    "    \n",
    "    # Stacked bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    agents = error_analysis.index\n",
    "    x = np.arange(len(agents))\n",
    "    \n",
    "    p1 = ax.bar(x, error_analysis['passed_count'], label='Passed', color='green', alpha=0.7)\n",
    "    p2 = ax.bar(x, error_analysis['failed_count'], bottom=error_analysis['passed_count'],\n",
    "               label='Failed', color='orange', alpha=0.7)\n",
    "    p3 = ax.bar(x, error_analysis['error_count'], \n",
    "               bottom=error_analysis['passed_count'] + error_analysis['failed_count'],\n",
    "               label='Error', color='red', alpha=0.7)\n",
    "    \n",
    "    ax.set_ylabel('Number of Test Cases', fontsize=12)\n",
    "    ax.set_xlabel('Agent', fontsize=12)\n",
    "    ax.set_title('Test Execution Results by Agent', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(agents)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/exp1_error_rates.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Tests\n",
    "\n",
    "Test whether differences between agents are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_combined' in locals() and len(df_combined['agent'].unique()) > 1:\n",
    "    from scipy.stats import chi2_contingency, wilcoxon\n",
    "    \n",
    "    # Chi-square test for pass/fail independence\n",
    "    print(\"Chi-square test for agent × pass/fail independence:\")\n",
    "    contingency_table = pd.crosstab(df_combined['agent'], df_combined['passed'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f\"  χ² = {chi2:.3f}, p = {p_value:.4f}, df = {dof}\")\n",
    "    print(f\"  Result: {'Significant' if p_value < 0.05 else 'Not significant'} at α=0.05\\n\")\n",
    "    \n",
    "    # Pairwise Wilcoxon tests for score distributions (if 2+ agents)\n",
    "    agents = df_combined['agent'].unique()\n",
    "    if len(agents) >= 2:\n",
    "        print(\"\\nPairwise Wilcoxon signed-rank tests for score distributions:\")\n",
    "        for i in range(len(agents)):\n",
    "            for j in range(i+1, len(agents)):\n",
    "                agent1_scores = df_combined[df_combined['agent'] == agents[i]]['score'].dropna()\n",
    "                agent2_scores = df_combined[df_combined['agent'] == agents[j]]['score'].dropna()\n",
    "                \n",
    "                # Match by test case for paired comparison\n",
    "                merged = df_combined[df_combined['agent'].isin([agents[i], agents[j]])].pivot_table(\n",
    "                    index=['name', 'MCP'], columns='agent', values='score'\n",
    "                ).dropna()\n",
    "                \n",
    "                if len(merged) > 0:\n",
    "                    stat, p = wilcoxon(merged[agents[i]], merged[agents[j]])\n",
    "                    print(f\"  {agents[i]} vs {agents[j]}: W = {stat:.3f}, p = {p:.4f}\")\n",
    "                    print(f\"    Result: {'Significant' if p < 0.05 else 'Not significant'} at α=0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_combined' in locals():\n",
    "    print(\"=\"*70)\n",
    "    print(\"KEY FINDINGS: Cross-Agent Comparison\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    overall_pass_rates = df_combined.groupby('agent')['passed'].mean() * 100\n",
    "    print(f\"\\n1. Overall Pass Rates:\")\n",
    "    for agent, rate in overall_pass_rates.items():\n",
    "        print(f\"   {agent}: {rate:.1f}%\")\n",
    "    \n",
    "    max_diff = overall_pass_rates.max() - overall_pass_rates.min()\n",
    "    print(f\"\\n   Max difference: {max_diff:.1f} percentage points\")\n",
    "    \n",
    "    if max_diff > 20:\n",
    "        print(\"   → Agent choice SIGNIFICANTLY affects performance\")\n",
    "    elif max_diff > 5:\n",
    "        print(\"   → Agent choice MODERATELY affects performance\")\n",
    "    else:\n",
    "        print(\"   → Agent choice has MINIMAL effect on performance\")\n",
    "    \n",
    "    print(f\"\\n2. Most Agent-Sensitive Case Groups:\")\n",
    "    if 'heatmap_data' in locals():\n",
    "        variance_by_group = heatmap_data.var(axis=1).sort_values(ascending=False)\n",
    "        for group, var in variance_by_group.head(3).items():\n",
    "            print(f\"   {group}: variance = {var:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **If agent choice matters significantly:** Investigate specific failure cases to understand why\n",
    "2. **If agent choice doesn't matter:** Current baseline results likely reflect true MCP capabilities\n",
    "3. **Export findings** for manuscript integration (see `notes/experiment_1_cross_agent_comparison.md`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
