{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 2: Run Cross-Model Evaluations\n\nThis notebook runs MCP evaluations using Codex agent with different LLM models.\n\n**Objective:** Determine whether model choice affects MCP retrieval performance when using the same coding agent.\n\n**Each evaluation takes 2-3 hours** and tests 25 cases × 4 MCP servers = 100 evaluations per model.\n\n**See:** `notes/EXPERIMENT_2_CROSS_MODEL.md` for detailed experimental design.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Environment Variables Required\n",
    "\n",
    "The evaluation scripts need several API keys and configuration:\n",
    "\n",
    "- `OPENAI_API_KEY`: For gpt-4o, gpt-5, gpt-4o-mini models\n",
    "- `PUBMED_EMAIL`: Required for PubMed API access\n",
    "- `PUBMED_API_KEY`: Required for pubmed-mcp server\n",
    "\n",
    "These are loaded from files:\n",
    "- `~/openai.key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "# Set working directory to project root\n",
    "project_root = Path.cwd().parent if 'notebook' in str(Path.cwd()) else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 2 Design\n\n**Independent Variable:** Underlying LLM model used by Codex agent\n- gpt-5 (baseline from Experiment 1)\n- gpt-5-mini (smaller/faster model)\n- gpt-5-nano (smallest/cheapest model)\n\n**Controlled Variables:**\n- Same agent: Codex CLI\n- Same MCP servers: artl, simple-pubmed, biomcp, pubmed-mcp\n- Same test cases: 25 cases\n- Same threshold: 0.9\n\n**Total evaluations:** 3 models × 4 MCPs × 25 cases = 300 evaluations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Configurations\n\nEach model uses a different config file in `project/generated/`:\n\n| Model | Config File | \n|-------|-------------|\n| gpt-5 | `literature_mcp_eval_config_codex_gpt5.yaml` |\n| gpt-5-mini | `literature_mcp_eval_config_codex_gpt5_mini.yaml` |\n| gpt-5-nano | `literature_mcp_eval_config_codex_gpt5_nano.yaml` |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Run Codex + gpt-5 Evaluation (Baseline)\n\n**Note:** We can reuse the results from Experiment 1 (`results/compare_agents/codex_20251208.yaml`)\n\n**Command (if running fresh):**\n\n```bash\n#!/bin/bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/generated/literature_mcp_eval_config_codex_gpt5.yaml \\\n  -o results/compare_models/codex_gpt5_$(date +%Y%m%d).yaml\n```\n\n**Duration:** ~2-3 hours\n\n**Output:** `results/compare_models/codex_gpt5_YYYYMMDD.yaml`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if gpt-5 evaluation already exists\ngpt5_result = f\"results/compare_models/codex_gpt5_{datetime.now().strftime('%Y%m%d')}.yaml\"\n\nif os.path.exists(gpt5_result):\n    print(f\"✓ gpt-5 evaluation already completed: {gpt5_result}\")\n    # Show brief summary\n    with open(gpt5_result, 'r') as f:\n        results = yaml.safe_load(f)\n        if results and 'results' in results:\n            count = len(results['results'])\n            passed = sum(1 for r in results['results'] if r.get('passed', False))\n            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\nelse:\n    print(\"⚠️  gpt-5 evaluation not found.\")\n    print(\"    You can copy from Experiment 1: cp results/compare_agents/codex_20251208.yaml results/compare_models/codex_gpt5_20251209.yaml\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Run Codex + gpt-5-mini Evaluation\n\n**Command:**\n\n```bash\n#!/bin/bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/generated/literature_mcp_eval_config_codex_gpt5_mini.yaml \\\n  -o results/compare_models/codex_gpt5_mini_$(date +%Y%m%d).yaml\n```\n\n**Duration:** ~2-3 hours\n\n**Output:** `results/compare_models/codex_gpt5_mini_YYYYMMDD.yaml`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if gpt-5-mini evaluation already exists\ngpt5_mini_result = f\"results/compare_models/codex_gpt5_mini_{datetime.now().strftime('%Y%m%d')}.yaml\"\n\nif os.path.exists(gpt5_mini_result):\n    print(f\"✓ gpt-5-mini evaluation already completed: {gpt5_mini_result}\")\n    with open(gpt5_mini_result, 'r') as f:\n        results = yaml.safe_load(f)\n        if results and 'results' in results:\n            count = len(results['results'])\n            passed = sum(1 for r in results['results'] if r.get('passed', False))\n            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\nelse:\n    print(\"⚠️  gpt-5-mini evaluation not found.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Run Codex + gpt-5-nano Evaluation\n\n**Command:**\n\n```bash\n#!/bin/bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/generated/literature_mcp_eval_config_codex_gpt5_nano.yaml \\\n  -o results/compare_models/codex_gpt5_nano_$(date +%Y%m%d).yaml\n```\n\n**Duration:** ~2-3 hours\n\n**Output:** `results/compare_models/codex_gpt5_nano_YYYYMMDD.yaml`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if gpt-5-nano evaluation already exists\ngpt5_nano_result = f\"results/compare_models/codex_gpt5_nano_{datetime.now().strftime('%Y%m%d')}.yaml\"\n\nif os.path.exists(gpt5_nano_result):\n    print(f\"✓ gpt-5-nano evaluation already completed: {gpt5_nano_result}\")\n    with open(gpt5_nano_result, 'r') as f:\n        results = yaml.safe_load(f)\n        if results and 'results' in results:\n            count = len(results['results'])\n            passed = sum(1 for r in results['results'] if r.get('passed', False))\n            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\nelse:\n    print(\"⚠️  gpt-5-nano evaluation not found.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check All Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from glob import glob\n\n# Find all result files in compare_models\nresult_files = sorted(glob(\"results/compare_models/codex_*.yaml\"))\n\nprint(\"Experiment 2 Evaluation Results\")\nprint(\"=\" * 70)\n\nmodels_found = {}\nfor f in result_files:\n    try:\n        # Get file size first to skip empty files\n        if os.path.getsize(f) == 0:\n            print(f\"\\n⚠️  {Path(f).name}: EMPTY FILE (evaluation incomplete or failed)\")\n            continue\n            \n        with open(f, 'r') as file:\n            data = yaml.safe_load(file)\n            if data and 'results' in data:\n                filename = Path(f).stem\n                # Extract model name: codex_MODEL_DATE.yaml\n                parts = filename.split('_')\n                if len(parts) >= 3:\n                    model = '_'.join(parts[1:-1])  # Everything between 'codex' and date\n                    date = parts[-1]\n                else:\n                    model = parts[1] if len(parts) > 1 else 'unknown'\n                    date = parts[2] if len(parts) > 2 else 'unknown'\n                \n                count = len(data['results'])\n                passed = sum(1 for r in data['results'] if r.get('passed', False))\n                pass_rate = (passed / count * 100) if count > 0 else 0\n                \n                models_found[model] = True\n                \n                print(f\"\\n{model.upper()} ({date}):\")\n                print(f\"  File: {f}\")\n                print(f\"  Tests: {count}\")\n                print(f\"  Passed: {passed}\")\n                print(f\"  Pass rate: {pass_rate:.1f}%\")\n    except Exception as e:\n        print(f\"\\nError reading {f}: {e}\")\n\nprint(\"\\n\" + \"=\" * 70)\n\n# Check completeness\nexpected_models = {'gpt5', 'gpt5_mini', 'gpt5_nano'}\nmissing_models = expected_models - set(models_found.keys())\n\nif missing_models:\n    print(f\"\\n⚠️  Missing evaluations for: {', '.join(missing_models)}\")\nelse:\n    print(\"\\n✓ All model evaluations complete!\")\n    print(\"\\nNext step: Run analysis in experiment_2_cross_model_analysis.ipynb\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations from Notebook (Alternative)\n",
    "\n",
    "If you want to run evaluations directly from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model_name, config_file, output_file):\n",
    "    \"\"\"\n",
    "    Run a metacoder evaluation for a specific model configuration.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Display name for the model\n",
    "        config_file: Path to YAML config file\n",
    "        output_file: Path for results output\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import subprocess\n",
    "    \n",
    "    # Set up environment\n",
    "    env = os.environ.copy()\n",
    "    \n",
    "    # Load API keys\n",
    "    with open(Path.home() / 'openai.key', 'r') as f:\n",
    "        env['OPENAI_API_KEY'] = f.read().strip()\n",
    "    \n",
    "    # PubMed credentials\n",
    "    env['PUBMED_EMAIL'] = 'justinreese@lbl.gov'\n",
    "    env['PUBMED_API_KEY'] = '01eec0a16472164c6d69163bd28368311808'\n",
    "    \n",
    "    # Remove old output file if exists\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "        print(f\"Removed old output: {output_file}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(f\"\\nStarting {model_name} evaluation...\")\n",
    "    print(f\"Config: {config_file}\")\n",
    "    print(f\"Output: {output_file}\")\n",
    "    print(f\"This will take 2-3 hours...\\n\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['uv', 'run', 'metacoder', 'eval', config_file, '-o', output_file],\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ {model_name} evaluation complete!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"✗ {model_name} evaluation failed:\")\n",
    "        print(result.stderr)\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# run_evaluation(\n",
    "#     model_name='gpt-4o-mini',\n",
    "#     config_file='project/literature_mcp_eval_config_goose_gpt4o_mini.yaml',\n",
    "#     output_file=f'results/compare_models/goose_gpt4o_mini_{datetime.now().strftime(\"%Y%m%d\")}.yaml'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **Empty result files**: Check if evaluation crashed mid-run\n",
    "   - Look for error logs\n",
    "   - Verify API keys are valid\n",
    "   - Check MCP server configurations\n",
    "\n",
    "2. **API rate limits**: OpenAI may throttle requests\n",
    "   - Evaluations automatically retry with backoff\n",
    "   - Check API usage dashboards\n",
    "\n",
    "3. **PubMed MCP failures**:\n",
    "   - Verify `PUBMED_EMAIL` is set\n",
    "   - Verify `PUBMED_API_KEY` is set (required for pubmed-mcp)\n",
    "\n",
    "### Monitoring Long-Running Evaluations\n",
    "\n",
    "```bash\n",
    "# Watch for new results being written\n",
    "watch -n 60 'wc -l results/compare_models/goose_*.yaml'\n",
    "\n",
    "# Check if metacoder is running\n",
    "ps aux | grep metacoder\n",
    "\n",
    "# Tail output if running in background\n",
    "tail -f nohup.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once all evaluations are complete:\n",
    "\n",
    "1. **Run analysis**: Open `experiment_2_cross_model_analysis.ipynb`\n",
    "2. **Generate figures**: The analysis notebook creates publication-ready plots\n",
    "3. **Compare with Experiment 1**: Cross-reference agent vs. model effects\n",
    "4. **Update manuscript**: Integrate findings into Results section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}