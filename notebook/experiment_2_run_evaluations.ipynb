{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Run Cross-Model Evaluations\n",
    "\n",
    "This notebook runs MCP evaluations using Goose agent with different LLM models.\n",
    "\n",
    "**Objective:** Determine whether model choice affects MCP retrieval performance when using the same coding agent.\n",
    "\n",
    "**Each evaluation takes 2-3 hours** and tests 25 cases × 4 MCP servers = 100 evaluations per model.\n",
    "\n",
    "**See:** `notes/EXPERIMENT_2_CROSS_MODEL.md` for detailed experimental design.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Environment Variables Required\n",
    "\n",
    "The evaluation scripts need several API keys and configuration:\n",
    "\n",
    "- `OPENAI_API_KEY`: For gpt-4o, gpt-5, gpt-4o-mini models\n",
    "- `PUBMED_EMAIL`: Required for PubMed API access\n",
    "- `PUBMED_API_KEY`: Required for pubmed-mcp server\n",
    "\n",
    "These are loaded from files:\n",
    "- `~/openai.key`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "# Set working directory to project root\n",
    "project_root = Path.cwd().parent if 'notebook' in str(Path.cwd()) else Path.cwd()\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 Design\n",
    "\n",
    "**Independent Variable:** Underlying LLM model used by Goose agent\n",
    "- gpt-4o (baseline from Experiment 1)\n",
    "- gpt-5 (latest OpenAI flagship)\n",
    "- gpt-4o-mini (smaller/cheaper OpenAI model)\n",
    "\n",
    "**Controlled Variables:**\n",
    "- Same agent: Goose CLI\n",
    "- Same MCP servers: artl, simple-pubmed, biomcp, pubmed-mcp\n",
    "- Same test cases: 25 cases\n",
    "- Same threshold: 0.9\n",
    "\n",
    "**Total evaluations:** 3 models × 4 MCPs × 25 cases = 300 evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "\n",
    "Each model uses a different config file in `project/`:\n",
    "\n",
    "| Model | Config File | Environment Variables |\n",
    "|-------|-------------|----------------------|\n",
    "| gpt-4o | `literature_mcp_eval_config.yaml` | `GOOSE_MODEL=gpt-4o`<br>`GOOSE_PROVIDER=openai` |\n",
    "| gpt-5 | `literature_mcp_eval_config_goose_gpt5.yaml` | `GOOSE_MODEL=gpt-5`<br>`GOOSE_PROVIDER=openai` |\n",
    "| gpt-4o-mini | `literature_mcp_eval_config_goose_gpt4o_mini.yaml` | `GOOSE_MODEL=gpt-4o-mini`<br>`GOOSE_PROVIDER=openai` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Goose + gpt-4o Evaluation (Baseline)\n",
    "\n",
    "**Command:**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "cd /Users/jtr4v/PythonProject/mcp_literature_eval\n",
    "export OPENAI_API_KEY=$(cat ~/openai.key)\n",
    "export PUBMED_EMAIL=justinreese@lbl.gov\n",
    "export PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\n",
    "rm -f results/compare_models/goose_gpt4o_$(date +%Y%m%d).yaml\n",
    "uv run metacoder eval project/literature_mcp_eval_config.yaml \\\n",
    "  -o results/compare_models/goose_gpt4o_$(date +%Y%m%d).yaml\n",
    "```\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_models/goose_gpt4o_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gpt-4o evaluation already exists\n",
    "gpt4o_result = f\"results/compare_models/goose_gpt4o_{datetime.now().strftime('%Y%m%d')}.yaml\"\n",
    "\n",
    "if os.path.exists(gpt4o_result):\n",
    "    print(f\"✓ gpt-4o evaluation already completed: {gpt4o_result}\")\n",
    "    # Show brief summary\n",
    "    with open(gpt4o_result, 'r') as f:\n",
    "        results = yaml.safe_load(f)\n",
    "        if results and 'results' in results:\n",
    "            count = len(results['results'])\n",
    "            passed = sum(1 for r in results['results'] if r.get('passed', False))\n",
    "            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠️  gpt-4o evaluation not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Goose + gpt-5 Evaluation\n",
    "\n",
    "**Command:**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "cd /Users/jtr4v/PythonProject/mcp_literature_eval\n",
    "export OPENAI_API_KEY=$(cat ~/openai.key)\n",
    "export PUBMED_EMAIL=justinreese@lbl.gov\n",
    "export PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\n",
    "rm -f results/compare_models/goose_gpt5_$(date +%Y%m%d).yaml\n",
    "uv run metacoder eval project/literature_mcp_eval_config_goose_gpt5.yaml \\\n",
    "  -o results/compare_models/goose_gpt5_$(date +%Y%m%d).yaml\n",
    "```\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_models/goose_gpt5_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gpt-5 evaluation already exists\n",
    "gpt5_result = f\"results/compare_models/goose_gpt5_{datetime.now().strftime('%Y%m%d')}.yaml\"\n",
    "\n",
    "if os.path.exists(gpt5_result):\n",
    "    print(f\"✓ gpt-5 evaluation already completed: {gpt5_result}\")\n",
    "    with open(gpt5_result, 'r') as f:\n",
    "        results = yaml.safe_load(f)\n",
    "        if results and 'results' in results:\n",
    "            count = len(results['results'])\n",
    "            passed = sum(1 for r in results['results'] if r.get('passed', False))\n",
    "            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠️  gpt-5 evaluation not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Goose + gpt-4o-mini Evaluation\n",
    "\n",
    "**Command:**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "cd /Users/jtr4v/PythonProject/mcp_literature_eval\n",
    "export OPENAI_API_KEY=$(cat ~/openai.key)\n",
    "export PUBMED_EMAIL=justinreese@lbl.gov\n",
    "export PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\n",
    "rm -f results/compare_models/goose_gpt4o_mini_$(date +%Y%m%d).yaml\n",
    "uv run metacoder eval project/literature_mcp_eval_config_goose_gpt4o_mini.yaml \\\n",
    "  -o results/compare_models/goose_gpt4o_mini_$(date +%Y%m%d).yaml\n",
    "```\n",
    "\n",
    "**Duration:** ~2-3 hours\n",
    "\n",
    "**Output:** `results/compare_models/goose_gpt4o_mini_YYYYMMDD.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gpt-4o-mini evaluation already exists\n",
    "gpt4o_mini_result = f\"results/compare_models/goose_gpt4o_mini_{datetime.now().strftime('%Y%m%d')}.yaml\"\n",
    "\n",
    "if os.path.exists(gpt4o_mini_result):\n",
    "    print(f\"✓ gpt-4o-mini evaluation already completed: {gpt4o_mini_result}\")\n",
    "    with open(gpt4o_mini_result, 'r') as f:\n",
    "        results = yaml.safe_load(f)\n",
    "        if results and 'results' in results:\n",
    "            count = len(results['results'])\n",
    "            passed = sum(1 for r in results['results'] if r.get('passed', False))\n",
    "            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠️  gpt-4o-mini evaluation not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check All Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Find all result files in compare_models\n",
    "result_files = sorted(glob(\"results/compare_models/goose_*.yaml\"))\n",
    "\n",
    "print(\"Experiment 2 Evaluation Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models_found = {}\n",
    "for f in result_files:\n",
    "    try:\n",
    "        # Get file size first to skip empty files\n",
    "        if os.path.getsize(f) == 0:\n",
    "            print(f\"\\n⚠️  {Path(f).name}: EMPTY FILE (evaluation incomplete or failed)\")\n",
    "            continue\n",
    "            \n",
    "        with open(f, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            if data and 'results' in data:\n",
    "                filename = Path(f).stem\n",
    "                # Extract model name: goose_MODEL_DATE.yaml\n",
    "                parts = filename.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    model = '_'.join(parts[1:-1])  # Everything between 'goose' and date\n",
    "                    date = parts[-1]\n",
    "                else:\n",
    "                    model = parts[1] if len(parts) > 1 else 'unknown'\n",
    "                    date = parts[2] if len(parts) > 2 else 'unknown'\n",
    "                \n",
    "                count = len(data['results'])\n",
    "                passed = sum(1 for r in data['results'] if r.get('passed', False))\n",
    "                pass_rate = (passed / count * 100) if count > 0 else 0\n",
    "                \n",
    "                models_found[model] = True\n",
    "                \n",
    "                print(f\"\\n{model.upper()} ({date}):\")\n",
    "                print(f\"  File: {f}\")\n",
    "                print(f\"  Tests: {count}\")\n",
    "                print(f\"  Passed: {passed}\")\n",
    "                print(f\"  Pass rate: {pass_rate:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError reading {f}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Check completeness\n",
    "expected_models = {'gpt4o', 'gpt5', 'gpt4o_mini'}\n",
    "missing_models = expected_models - set(models_found.keys())\n",
    "\n",
    "if missing_models:\n",
    "    print(f\"\\n⚠️  Missing evaluations for: {', '.join(missing_models)}\")\n",
    "else:\n",
    "    print(\"\\n✓ All model evaluations complete!\")\n",
    "    print(\"\\nNext step: Run analysis in experiment_2_cross_model_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations from Notebook (Alternative)\n",
    "\n",
    "If you want to run evaluations directly from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model_name, config_file, output_file):\n",
    "    \"\"\"\n",
    "    Run a metacoder evaluation for a specific model configuration.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Display name for the model\n",
    "        config_file: Path to YAML config file\n",
    "        output_file: Path for results output\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import subprocess\n",
    "    \n",
    "    # Set up environment\n",
    "    env = os.environ.copy()\n",
    "    \n",
    "    # Load API keys\n",
    "    with open(Path.home() / 'openai.key', 'r') as f:\n",
    "        env['OPENAI_API_KEY'] = f.read().strip()\n",
    "    \n",
    "    # PubMed credentials\n",
    "    env['PUBMED_EMAIL'] = 'justinreese@lbl.gov'\n",
    "    env['PUBMED_API_KEY'] = '01eec0a16472164c6d69163bd28368311808'\n",
    "    \n",
    "    # Remove old output file if exists\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "        print(f\"Removed old output: {output_file}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(f\"\\nStarting {model_name} evaluation...\")\n",
    "    print(f\"Config: {config_file}\")\n",
    "    print(f\"Output: {output_file}\")\n",
    "    print(f\"This will take 2-3 hours...\\n\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['uv', 'run', 'metacoder', 'eval', config_file, '-o', output_file],\n",
    "        env=env,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✓ {model_name} evaluation complete!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"✗ {model_name} evaluation failed:\")\n",
    "        print(result.stderr)\n",
    "        return False\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# run_evaluation(\n",
    "#     model_name='gpt-4o-mini',\n",
    "#     config_file='project/literature_mcp_eval_config_goose_gpt4o_mini.yaml',\n",
    "#     output_file=f'results/compare_models/goose_gpt4o_mini_{datetime.now().strftime(\"%Y%m%d\")}.yaml'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **Empty result files**: Check if evaluation crashed mid-run\n",
    "   - Look for error logs\n",
    "   - Verify API keys are valid\n",
    "   - Check MCP server configurations\n",
    "\n",
    "2. **API rate limits**: OpenAI may throttle requests\n",
    "   - Evaluations automatically retry with backoff\n",
    "   - Check API usage dashboards\n",
    "\n",
    "3. **PubMed MCP failures**:\n",
    "   - Verify `PUBMED_EMAIL` is set\n",
    "   - Verify `PUBMED_API_KEY` is set (required for pubmed-mcp)\n",
    "\n",
    "### Monitoring Long-Running Evaluations\n",
    "\n",
    "```bash\n",
    "# Watch for new results being written\n",
    "watch -n 60 'wc -l results/compare_models/goose_*.yaml'\n",
    "\n",
    "# Check if metacoder is running\n",
    "ps aux | grep metacoder\n",
    "\n",
    "# Tail output if running in background\n",
    "tail -f nohup.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once all evaluations are complete:\n",
    "\n",
    "1. **Run analysis**: Open `experiment_2_cross_model_analysis.ipynb`\n",
    "2. **Generate figures**: The analysis notebook creates publication-ready plots\n",
    "3. **Compare with Experiment 1**: Cross-reference agent vs. model effects\n",
    "4. **Update manuscript**: Integrate findings into Results section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
