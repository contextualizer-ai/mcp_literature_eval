{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 2: Run Cross-Model Evaluations\n\nThis notebook runs MCP evaluations using Codex agent with different LLM models.\n\n**Objective:** Determine whether model choice affects MCP retrieval performance when using the same coding agent.\n\n**Each evaluation takes 2-3 hours** and tests 25 cases √ó 4 MCP servers = 100 evaluations per model.\n\n**See:** `notes/EXPERIMENT_2_CROSS_MODEL.md` for detailed experimental design.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup\n\n### Environment Variables Required\n\nThe evaluation scripts need several API keys and configuration:\n\n- `OPENAI_API_KEY`: For Codex models (gpt-5.1-codex-max, gpt-5.1-codex-mini, gpt-5.1-codex)\n- `PUBMED_EMAIL`: Required for PubMed API access\n- `PUBMED_API_KEY`: Required for pubmed-mcp server\n\nThese are loaded from files:\n- `~/openai.key.another`\n\n### MCP-Only Mode\n\nAll evaluations run with `disable_shell_tool: true` to prevent filesystem access and ensure fair comparison using only MCP tools."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nimport yaml\nimport shutil\n\n# Set working directory to project root\nproject_root = Path.cwd().parent if 'notebook' in str(Path.cwd()) else Path.cwd()\nos.chdir(project_root)\nprint(f\"Working directory: {os.getcwd()}\")\n\ndef run_isolated_eval(model_name: str, config_path: str, output_name: str, background: bool = False):\n    \"\"\"\n    Run Codex evaluation in isolated /tmp directory with shell_tool disabled.\n    \n    Args:\n        model_name: Display name for the model (e.g., 'gpt5', 'gpt51_codex_mini')\n        config_path: Path to config file relative to project root\n        output_name: Base name for output file (e.g., 'codex_gpt5_mcp_only')\n        background: If True, run in background\n    \n    Returns:\n        Tuple of (isolated_dir, output_file, process or None)\n    \"\"\"\n    # Create isolated directory in /tmp\n    isolated_dir = Path(f\"/tmp/mcp_eval_isolated_codex_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n    isolated_dir.mkdir(parents=True, exist_ok=True)\n    \n    print(f\"=== Creating isolated environment: {isolated_dir} ===\")\n    \n    # Copy ONLY the config file\n    config_file = project_root / config_path\n    config_basename = config_file.name\n    isolated_config = isolated_dir / config_basename\n    shutil.copy(config_file, isolated_config)\n    \n    print(f\"‚úì Copied config: {config_basename}\")\n    \n    # Create isolated workdir\n    isolated_workdir = isolated_dir / \"eval_workdir\"\n    isolated_workdir.mkdir(exist_ok=True)\n    \n    print(f\"‚úì Created isolated workdir: {isolated_workdir}\")\n    \n    # Set output file\n    output_file = project_root / f\"results/compare_models/{output_name}_{datetime.now().strftime('%Y%m%d')}.yaml\"\n    \n    print(f\"=== Running evaluation ===\")\n    print(f\"Model: {model_name}\")\n    print(f\"Config: {config_basename}\")\n    print(f\"Run dir: {isolated_dir}\")\n    print(f\"Workdir: {isolated_workdir}\")\n    print(f\"Output: {output_file}\")\n    print(\"\")\n    \n    # Set environment variables\n    env = os.environ.copy()\n    env[\"OPENAI_API_KEY\"] = open(Path.home() / \"openai.key.another\").read().strip()\n    env[\"PUBMED_EMAIL\"] = \"justinreese@lbl.gov\"\n    env[\"PUBMED_API_KEY\"] = \"01eec0a16472164c6d69163bd28368311808\"\n    \n    # Use project's venv python\n    venv_python = project_root / \".venv/bin/python\"\n    \n    cmd = [\n        str(venv_python), \"-m\", \"metacoder.metacoder\", \"eval\",\n        str(isolated_config),\n        \"--workdir\", str(isolated_workdir),\n        \"-o\", str(output_file)\n    ]\n    \n    if background:\n        print(f\"üöÄ Starting evaluation in background...\")\n        process = subprocess.Popen(\n            cmd,\n            cwd=isolated_dir,\n            env=env,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        return isolated_dir, output_file, process\n    else:\n        print(f\"üöÄ Starting evaluation (this will take 2-3 hours)...\")\n        result = subprocess.run(\n            cmd,\n            cwd=isolated_dir,\n            env=env,\n            capture_output=True,\n            text=True\n        )\n        \n        if result.returncode != 0:\n            print(f\"‚ùå Evaluation failed with return code {result.returncode}\")\n            print(f\"STDERR: {result.stderr}\")\n            raise RuntimeError(f\"Evaluation failed: {result.stderr}\")\n        \n        print(f\"‚úÖ Evaluation complete!\")\n        print(f\"Output saved to: {output_file}\")\n        \n        return isolated_dir, output_file, None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment 2 Design\n\n**Independent Variable:** Underlying LLM model used by Codex agent\n- gpt-5.1-codex-max (baseline from Experiment 1)\n- gpt-5.1-codex-mini (smaller/faster model)\n- gpt-5.1-codex (standard Codex model)\n\n**Controlled Variables:**\n- Same agent: Codex CLI\n- Same MCP servers: artl, simple-pubmed, biomcp, pubmed-mcp\n- Same test cases: 25 cases\n- Same threshold: 0.9\n\n**Total evaluations:** 3 models √ó 4 MCPs √ó 25 cases = 300 evaluations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Configurations\n\nEach model uses a different config file in `project/generated/`:\n\n| Model | Config File | \n|-------|-------------|\n| gpt-5.1-codex-max | `literature_mcp_eval_config_codex_gpt5.yaml` |\n| gpt-5.1-codex-mini | `literature_mcp_eval_config_codex_gpt51_codex_mini.yaml` |\n| gpt-5.1-codex | `literature_mcp_eval_config_codex_gpt51_codex.yaml` |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Run Codex + gpt-5 Evaluation (Baseline)\n\n**Note:** We can reuse the results from Experiment 1 (`results/compare_agents/codex_20251216.yaml`)\n\n**Command (if running fresh):**\n\n```bash\n#!/bin/bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/generated/literature_mcp_eval_config_codex_gpt5.yaml \\\n  -o results/compare_models/codex_gpt5_$(date +%Y%m%d).yaml\n```\n\n**Duration:** ~2-3 hours\n\n**Output:** `results/compare_models/codex_gpt5_YYYYMMDD.yaml`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if gpt-5 evaluation already exists\ngpt5_result = f\"results/compare_models/codex_gpt5_{datetime.now().strftime('%Y%m%d')}.yaml\"\n\nif os.path.exists(gpt5_result):\n    print(f\"‚úì gpt-5 evaluation already completed: {gpt5_result}\")\n    # Show brief summary\n    with open(gpt5_result, 'r') as f:\n        results = yaml.safe_load(f)\n        if results and 'results' in results:\n            count = len(results['results'])\n            passed = sum(1 for r in results['results'] if r.get('passed', False))\n            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\nelse:\n    print(\"‚ö†Ô∏è  gpt-5 evaluation not found.\")\n    print(\"    You can copy from Experiment 1: cp results/compare_agents/codex_20251216.yaml results/compare_models/codex_gpt5_20251209.yaml\")"
  },
  {
   "cell_type": "code",
   "source": "# Run gpt-5.1-codex-max evaluation (if not already done)\n# Uncomment the line below to run:\n\n# run_isolated_eval(\n#     model_name='gpt5_max',\n#     config_path='project/generated/literature_mcp_eval_config_codex_gpt5.yaml',\n#     output_name='codex_gpt5_max_mcp_only'\n# )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Run Codex + gpt-5.1-codex-mini Evaluation\n\n**Command:**\n\n```bash\n#!/bin/bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/generated/literature_mcp_eval_config_codex_gpt51_codex_mini.yaml \\\n  -o results/compare_models/codex_gpt51_codex_mini_$(date +%Y%m%d).yaml\n```\n\n**Duration:** ~2-3 hours\n\n**Output:** `results/compare_models/codex_gpt51_codex_mini_YYYYMMDD.yaml`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if gpt-5-mini evaluation already exists\ngpt5_mini_result = f\"results/compare_models/codex_gpt5_mini_{datetime.now().strftime('%Y%m%d')}.yaml\"\n\nif os.path.exists(gpt5_mini_result):\n    print(f\"‚úì gpt-5-mini evaluation already completed: {gpt5_mini_result}\")\n    with open(gpt5_mini_result, 'r') as f:\n        results = yaml.safe_load(f)\n        if results and 'results' in results:\n            count = len(results['results'])\n            passed = sum(1 for r in results['results'] if r.get('passed', False))\n            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\nelse:\n    print(\"‚ö†Ô∏è  gpt-5-mini evaluation not found.\")"
  },
  {
   "cell_type": "code",
   "source": "# Run gpt-5.1-codex-mini evaluation (if not already done)\n# Uncomment the line below to run:\n\n# run_isolated_eval(\n#     model_name='gpt51_codex_mini',\n#     config_path='project/generated/literature_mcp_eval_config_codex_gpt51_codex_mini.yaml',\n#     output_name='codex_gpt51_codex_mini_mcp_only'\n# )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Run Codex + gpt-5.1-codex Evaluation\n\n**Command:**\n\n```bash\n#!/bin/bash\ncd /Users/jtr4v/PythonProject/mcp_literature_eval\nexport OPENAI_API_KEY=$(cat ~/openai.key)\nexport PUBMED_EMAIL=justinreese@lbl.gov\nexport PUBMED_API_KEY=01eec0a16472164c6d69163bd28368311808\nuv run metacoder eval project/generated/literature_mcp_eval_config_codex_gpt51_codex.yaml \\\n  -o results/compare_models/codex_gpt51_codex_$(date +%Y%m%d).yaml\n```\n\n**Duration:** ~2-3 hours\n\n**Output:** `results/compare_models/codex_gpt51_codex_YYYYMMDD.yaml`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if gpt-5-nano evaluation already exists\ngpt5_nano_result = f\"results/compare_models/codex_gpt5_nano_{datetime.now().strftime('%Y%m%d')}.yaml\"\n\nif os.path.exists(gpt5_nano_result):\n    print(f\"‚úì gpt-5-nano evaluation already completed: {gpt5_nano_result}\")\n    with open(gpt5_nano_result, 'r') as f:\n        results = yaml.safe_load(f)\n        if results and 'results' in results:\n            count = len(results['results'])\n            passed = sum(1 for r in results['results'] if r.get('passed', False))\n            print(f\"  Tests: {count}, Passed: {passed} ({passed/count*100:.1f}%)\")\nelse:\n    print(\"‚ö†Ô∏è  gpt-5-nano evaluation not found.\")"
  },
  {
   "cell_type": "code",
   "source": "# Run gpt-5.1-codex evaluation (if not already done)\n# Uncomment the line below to run:\n\n# run_isolated_eval(\n#     model_name='gpt51_codex',\n#     config_path='project/generated/literature_mcp_eval_config_codex_gpt51_codex.yaml',\n#     output_name='codex_gpt51_codex_mcp_only'\n# )",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check All Evaluation Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from glob import glob\n\n# Find all result files in compare_models\nresult_files = sorted(glob(\"results/compare_models/codex_*.yaml\"))\n\nprint(\"Experiment 2 Evaluation Results\")\nprint(\"=\" * 70)\n\nmodels_found = {}\nfor f in result_files:\n    try:\n        # Get file size first to skip empty files\n        if os.path.getsize(f) == 0:\n            print(f\"\\n‚ö†Ô∏è  {Path(f).name}: EMPTY FILE (evaluation incomplete or failed)\")\n            continue\n            \n        with open(f, 'r') as file:\n            data = yaml.safe_load(file)\n            if data and 'results' in data:\n                filename = Path(f).stem\n                # Extract model name: codex_MODEL_DATE.yaml\n                parts = filename.split('_')\n                if len(parts) >= 3:\n                    model = '_'.join(parts[1:-1])  # Everything between 'codex' and date\n                    date = parts[-1]\n                else:\n                    model = parts[1] if len(parts) > 1 else 'unknown'\n                    date = parts[2] if len(parts) > 2 else 'unknown'\n                \n                count = len(data['results'])\n                passed = sum(1 for r in data['results'] if r.get('passed', False))\n                pass_rate = (passed / count * 100) if count > 0 else 0\n                \n                models_found[model] = True\n                \n                print(f\"\\n{model.upper()} ({date}):\")\n                print(f\"  File: {f}\")\n                print(f\"  Tests: {count}\")\n                print(f\"  Passed: {passed}\")\n                print(f\"  Pass rate: {pass_rate:.1f}%\")\n    except Exception as e:\n        print(f\"\\nError reading {f}: {e}\")\n\nprint(\"\\n\" + \"=\" * 70)\n\n# Check completeness\nexpected_models = {'gpt5', 'gpt5_mini', 'gpt5_nano'}\nmissing_models = expected_models - set(models_found.keys())\n\nif missing_models:\n    print(f\"\\n‚ö†Ô∏è  Missing evaluations for: {', '.join(missing_models)}\")\nelse:\n    print(\"\\n‚úì All model evaluations complete!\")\n    print(\"\\nNext step: Run analysis in experiment_2_cross_model_analysis.ipynb\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations from Notebook (Alternative)\n",
    "\n",
    "If you want to run evaluations directly from this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DEPRECATED: Use run_isolated_eval() instead - it runs in /tmp with MCP-only mode\n# This function is kept for reference only\n\ndef run_evaluation(model_name, config_file, output_file):\n    \"\"\"\n    DEPRECATED: Use run_isolated_eval() instead.\n    \n    Run a metacoder evaluation for a specific model configuration.\n    \"\"\"\n    import os\n    from pathlib import Path\n    import subprocess\n    \n    # Set up environment\n    env = os.environ.copy()\n    \n    # Load API keys - use openai.key.another\n    with open(Path.home() / 'openai.key.another', 'r') as f:\n        env['OPENAI_API_KEY'] = f.read().strip()\n    \n    # PubMed credentials\n    env['PUBMED_EMAIL'] = 'justinreese@lbl.gov'\n    env['PUBMED_API_KEY'] = '01eec0a16472164c6d69163bd28368311808'\n    \n    # Remove old output file if exists\n    if os.path.exists(output_file):\n        os.remove(output_file)\n        print(f\"Removed old output: {output_file}\")\n    \n    # Run evaluation\n    print(f\"\\nStarting {model_name} evaluation...\")\n    print(f\"Config: {config_file}\")\n    print(f\"Output: {output_file}\")\n    print(f\"This will take 2-3 hours...\\n\")\n    \n    result = subprocess.run(\n        ['uv', 'run', 'metacoder', 'eval', config_file, '-o', output_file],\n        env=env,\n        capture_output=True,\n        text=True\n    )\n    \n    if result.returncode == 0:\n        print(f\"‚úì {model_name} evaluation complete!\")\n        return True\n    else:\n        print(f\"‚úó {model_name} evaluation failed:\")\n        print(result.stderr)\n        return False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **Empty result files**: Check if evaluation crashed mid-run\n",
    "   - Look for error logs\n",
    "   - Verify API keys are valid\n",
    "   - Check MCP server configurations\n",
    "\n",
    "2. **API rate limits**: OpenAI may throttle requests\n",
    "   - Evaluations automatically retry with backoff\n",
    "   - Check API usage dashboards\n",
    "\n",
    "3. **PubMed MCP failures**:\n",
    "   - Verify `PUBMED_EMAIL` is set\n",
    "   - Verify `PUBMED_API_KEY` is set (required for pubmed-mcp)\n",
    "\n",
    "### Monitoring Long-Running Evaluations\n",
    "\n",
    "```bash\n",
    "# Watch for new results being written\n",
    "watch -n 60 'wc -l results/compare_models/goose_*.yaml'\n",
    "\n",
    "# Check if metacoder is running\n",
    "ps aux | grep metacoder\n",
    "\n",
    "# Tail output if running in background\n",
    "tail -f nohup.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Once all evaluations are complete:\n",
    "\n",
    "1. **Run analysis**: Open `experiment_2_cross_model_analysis.ipynb`\n",
    "2. **Generate figures**: The analysis notebook creates publication-ready plots\n",
    "3. **Compare with Experiment 1**: Cross-reference agent vs. model effects\n",
    "4. **Update manuscript**: Integrate findings into Results section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}